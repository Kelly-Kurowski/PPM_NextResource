{
 "cells": [
  {
   "cell_type": "code",
   "id": "631123e1ee20e74c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:54:25.719309Z",
     "start_time": "2025-01-07T12:54:19.525044Z"
    }
   },
   "source": [
    "import pm4py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, LSTM\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6706363/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "312aa6e965ec36da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T09:48:50.133531Z",
     "start_time": "2024-12-27T09:48:50.130951Z"
    }
   },
   "source": [
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Set no limit for column width"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "e987ce9af8b41e93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:55:37.897685Z",
     "start_time": "2025-01-07T12:54:36.343620Z"
    }
   },
   "source": [
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    event_log = pm4py.convert_to_dataframe(log)\n",
    "\n",
    "    return event_log\n",
    "\n",
    "event_log = import_xes(\"/Users/6706363/Downloads/BPI Challenge 2017.xes\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6706363/Library/Python/3.9/lib/python/site-packages/pm4py/util/dt_parsing/parser.py:77: UserWarning: ISO8601 strings are not fully supported with strpfromiso for Python versions below 3.11\n",
      "  warnings.warn(\n",
      "/Users/6706363/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "parsing log, completed traces :: 100%|██████████| 31509/31509 [00:39<00:00, 803.64it/s] \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "759904c2e9c2e744",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:55:48.212758Z",
     "start_time": "2025-01-07T12:55:47.115338Z"
    }
   },
   "source": [
    "# Assuming event_log is your DataFrame\n",
    "df = event_log[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "\n",
    "# Sort by 'time:timestamp' and 'case:concept:name'\n",
    "df = df.sort_values(by=['case:concept:name', 'time:timestamp'])\n",
    "\n",
    "df.head(n=10)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             case:concept:name            concept:name org:resource  \\\n",
       "686058  Application_1000086665    A_Create Application       User_1   \n",
       "686059  Application_1000086665             A_Submitted       User_1   \n",
       "686060  Application_1000086665          W_Handle leads       User_1   \n",
       "686061  Application_1000086665          W_Handle leads       User_1   \n",
       "686062  Application_1000086665  W_Complete application       User_1   \n",
       "686063  Application_1000086665               A_Concept       User_1   \n",
       "686064  Application_1000086665  W_Complete application      User_14   \n",
       "686065  Application_1000086665  W_Complete application      User_14   \n",
       "686066  Application_1000086665              A_Accepted       User_5   \n",
       "686067  Application_1000086665          O_Create Offer       User_5   \n",
       "\n",
       "                         time:timestamp  \n",
       "686058 2016-08-03 15:57:21.673000+00:00  \n",
       "686059 2016-08-03 15:57:21.734000+00:00  \n",
       "686060 2016-08-03 15:57:21.963000+00:00  \n",
       "686061 2016-08-03 15:58:28.286000+00:00  \n",
       "686062 2016-08-03 15:58:28.293000+00:00  \n",
       "686063 2016-08-03 15:58:28.299000+00:00  \n",
       "686064 2016-08-04 13:39:29.557000+00:00  \n",
       "686065 2016-08-04 13:50:12.281000+00:00  \n",
       "686066 2016-08-05 13:57:07.419000+00:00  \n",
       "686067 2016-08-05 13:59:57.320000+00:00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>time:timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>686058</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>A_Create Application</td>\n",
       "      <td>User_1</td>\n",
       "      <td>2016-08-03 15:57:21.673000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686059</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>A_Submitted</td>\n",
       "      <td>User_1</td>\n",
       "      <td>2016-08-03 15:57:21.734000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686060</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>W_Handle leads</td>\n",
       "      <td>User_1</td>\n",
       "      <td>2016-08-03 15:57:21.963000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686061</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>W_Handle leads</td>\n",
       "      <td>User_1</td>\n",
       "      <td>2016-08-03 15:58:28.286000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686062</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>W_Complete application</td>\n",
       "      <td>User_1</td>\n",
       "      <td>2016-08-03 15:58:28.293000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686063</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>A_Concept</td>\n",
       "      <td>User_1</td>\n",
       "      <td>2016-08-03 15:58:28.299000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686064</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>W_Complete application</td>\n",
       "      <td>User_14</td>\n",
       "      <td>2016-08-04 13:39:29.557000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686065</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>W_Complete application</td>\n",
       "      <td>User_14</td>\n",
       "      <td>2016-08-04 13:50:12.281000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686066</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>A_Accepted</td>\n",
       "      <td>User_5</td>\n",
       "      <td>2016-08-05 13:57:07.419000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686067</th>\n",
       "      <td>Application_1000086665</td>\n",
       "      <td>O_Create Offer</td>\n",
       "      <td>User_5</td>\n",
       "      <td>2016-08-05 13:59:57.320000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "9a688b0f242377e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:56:00.538973Z",
     "start_time": "2025-01-07T12:55:57.606174Z"
    }
   },
   "source": [
    "def create_activity_resource_sequence(df, prefix_length):\n",
    "    sequences = []\n",
    "    grouped = df.groupby('case:concept:name')\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        activities = group['concept:name'].tolist()\n",
    "        resources = group['org:resource'].tolist()\n",
    "        \n",
    "        # Only include sequences with length >= prefix_length\n",
    "        if len(activities) < prefix_length:\n",
    "            # Remove the sequence (skip appending it to the list)\n",
    "            continue\n",
    "        \n",
    "        # Truncate to the desired prefix length\n",
    "        current_activities = activities[:prefix_length]\n",
    "        current_resources = resources[:prefix_length]  # Include all resources\n",
    "        \n",
    "        # Combine activities and resources into tuples (no changes for the last activity)\n",
    "        sequence = []\n",
    "        for i in range(len(current_activities)):\n",
    "            # For all activities, include both activity and resource\n",
    "            sequence.append((current_activities[i], current_resources[i]))\n",
    "        \n",
    "        # Add the valid sequence to the list\n",
    "        sequences.append(sequence)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Example usage\n",
    "sequences = create_activity_resource_sequence(df,5)\n",
    "\n",
    "# Initialize a set to store unique 'R' values\n",
    "unique_R = set()\n",
    "\n",
    "# Loop through the list of sequences and extract the 'R' values\n",
    "for sequence in sequences:\n",
    "    for item in sequence:\n",
    "        # item[1] is the second element (the part with 'R')\n",
    "        unique_R.add(item[1])\n",
    "\n",
    "# The length of the set will give the number of unique occurrences of 'R'\n",
    "print(len(unique_R))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "c77b67ef8b2d3ded",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T13:48:23.898028Z",
     "start_time": "2025-01-07T13:47:08.876241Z"
    }
   },
   "source": [
    "# Prepare the list of activities and resources\n",
    "activities = []\n",
    "resources = []\n",
    "\n",
    "# Loop through sequences to gather activities and resources\n",
    "for seq in sequences:\n",
    "    for i, item in enumerate(seq):\n",
    "        activity, resource = item  # Each item is (activity, resource)\n",
    "        # Replace NaN resource with 'none'\n",
    "        if pd.isna(resource):  # Check if the resource is NaN\n",
    "            resource = 'none'\n",
    "        activities.append(activity)\n",
    "        resources.append(resource)\n",
    "\n",
    "# Fit the OneHotEncoder to the unique activities and resources\n",
    "activity_encoder = OneHotEncoder() \n",
    "resource_encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder on unique activities and resources\n",
    "activity_encoder.fit([[activity] for activity in set(activities)])\n",
    "resource_encoder.fit([[resource] for resource in set(resources)])\n",
    "\n",
    "# Encode activities and resources\n",
    "encoded_sequences = []\n",
    "y_encoded = []  # List to store the one-hot encoded target resource for the last activity\n",
    "\n",
    "for seq in sequences:\n",
    "    activity_onehots = []\n",
    "    \n",
    "    # For each activity-resource pair, apply one-hot encoding\n",
    "    for i, item in enumerate(seq):\n",
    "        activity, resource = item\n",
    "        # Replace NaN resource with 'none' during encoding\n",
    "        if pd.isna(resource):  # Check if the resource is NaN\n",
    "            resource = 'none'\n",
    "        activity_onehot = activity_encoder.transform([[activity]]).toarray()\n",
    "        \n",
    "        # If it's the last item, we only encode the activity and store the resource for y\n",
    "        if i == len(seq) - 1:\n",
    "            # Add only the activity one-hot encoding\n",
    "            activity_onehots.append(activity_onehot)\n",
    "            # One-hot encode the resource and store it for prediction (y)\n",
    "            resource_onehot = resource_encoder.transform([[resource]]).toarray()\n",
    "            y_encoded.append(resource_onehot)  # Store the one-hot encoded resource\n",
    "        else:\n",
    "            # For all other activities, include both activity and resource one-hot encoding\n",
    "            resource_onehot = resource_encoder.transform([[resource]]).toarray()\n",
    "            encoded_sequence = np.hstack([activity_onehot, resource_onehot])\n",
    "            activity_onehots.append(encoded_sequence)\n",
    "    \n",
    "    # If there is more than one activity in the sequence, add the zero vector for the last resource\n",
    "    if len(seq) > 1:\n",
    "        last_activity_onehot = activity_onehots[-1]\n",
    "        last_resource_onehot = np.zeros(resource_onehot.shape)  # Zero vector for the last resource\n",
    "        activity_onehots[-1] = np.hstack([last_activity_onehot, last_resource_onehot])\n",
    "    \n",
    "    # Concatenate the encoded activities and resources for the full sequence\n",
    "    encoded_sequences.append(np.vstack(activity_onehots))\n",
    "\n",
    "X = np.array(encoded_sequences)\n",
    "y = np.array(y_encoded)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31509, 5, 125)\n",
      "(31509, 1, 118)\n",
      "Sample of original labels: [[[1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. ... 0. 0. 0.]]]\n",
      "Decoded unique classes: [0]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T12:57:49.910265Z",
     "start_time": "2025-01-07T12:57:49.667180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize KFold with 5 splits\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Initialize the model \n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with return_sequences=True\n",
    "    model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(50))\n",
    "    # Output Dense layer\n",
    "    model.add(Dense(143, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Store metrics from each fold\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Loop through the KFold splits\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Squeeze the target arrays to remove the extra dimension\n",
    "    y_train = y_train.squeeze(axis=1)\n",
    "    y_test = y_test.squeeze(axis=1)\n",
    "    \n",
    "    # Create the model for each fold\n",
    "    model = create_model()\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0, \n",
    "                        validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
    "    y_test_classes = np.argmax(y_test, axis=1)  # Ensure test labels are in class label format\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(y_pred_classes == y_test_classes)\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Store metrics\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Output average metrics\n",
    "print(f'Average Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')\n",
    "print(f'Average Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}')\n",
    "print(f'Average Recall: {np.mean(recalls):.4f} ± {np.std(recalls):.4f}')\n",
    "print(f'Average F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}')"
   ],
   "id": "bcef32dc7be43b5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 118), output.shape=(None, 143)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 38\u001B[0m\n\u001B[1;32m     35\u001B[0m model \u001B[38;5;241m=\u001B[39m create_model()\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Train the model with early stopping\u001B[39;00m\n\u001B[0;32m---> 38\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     39\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Make predictions\u001B[39;00m\n\u001B[1;32m     42\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/keras/src/backend/tensorflow/nn.py:660\u001B[0m, in \u001B[0;36mcategorical_crossentropy\u001B[0;34m(target, output, from_logits, axis)\u001B[0m\n\u001B[1;32m    658\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m e1, e2 \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(target\u001B[38;5;241m.\u001B[39mshape, output\u001B[38;5;241m.\u001B[39mshape):\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m e1 \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m e2 \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m e1 \u001B[38;5;241m!=\u001B[39m e2:\n\u001B[0;32m--> 660\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    661\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArguments `target` and `output` must have the same shape. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    662\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    663\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget.shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtarget\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, output.shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutput\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    664\u001B[0m         )\n\u001B[1;32m    666\u001B[0m output, from_logits \u001B[38;5;241m=\u001B[39m _get_logits(\n\u001B[1;32m    667\u001B[0m     output, from_logits, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSoftmax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcategorical_crossentropy\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    668\u001B[0m )\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m from_logits:\n",
      "\u001B[0;31mValueError\u001B[0m: Arguments `target` and `output` must have the same shape. Received: target.shape=(None, 118), output.shape=(None, 143)"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize KFold with 5 splits\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Initialize the model \n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with return_sequences=True\n",
    "    model.add(LSTM(50, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(50))\n",
    "    # Output Dense layer\n",
    "    model.add(Dense(118, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Store metrics from each fold\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Initialize EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Initialize list to store loss curves\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Loop through the KFold splits\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Squeeze the target arrays to remove the extra dimension\n",
    "    y_train = y_train.squeeze(axis=1)\n",
    "    y_test = y_test.squeeze(axis=1)\n",
    "    \n",
    "    # Create the model for each fold\n",
    "    model = create_model()\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0, \n",
    "                        validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "    \n",
    "    # Capture the training and validation losses\n",
    "    training_losses.append(history.history['loss'])\n",
    "    validation_losses.append(history.history['val_loss'])\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
    "    y_test_classes = np.argmax(y_test, axis=1)  # Ensure test labels are in class label format\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = np.mean(y_pred_classes == y_test_classes)\n",
    "    precision = precision_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test_classes, y_pred_classes, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Store metrics\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Output average metrics\n",
    "print(f'Average Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}')\n",
    "print(f'Average Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}')\n",
    "print(f'Average Recall: {np.mean(recalls):.4f} ± {np.std(recalls):.4f}')\n",
    "print(f'Average F1-Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}')\n",
    "\n",
    "# Plot the loss curve after all folds\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the loss for each fold\n",
    "for i in range(len(training_losses)):\n",
    "    plt.plot(training_losses[i], label=f'Train Fold {i+1}')\n",
    "    plt.plot(validation_losses[i], label=f'Validation Fold {i+1}')\n",
    "\n",
    "plt.title('Loss Curve for Each Fold')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print the prediction report\n",
    "print(classification_report(y_test_classes, y_pred_classes))    "
   ],
   "id": "aecd31650373b50a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
