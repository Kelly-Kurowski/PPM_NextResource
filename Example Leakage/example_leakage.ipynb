{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T14:48:51.416251Z",
     "start_time": "2025-05-15T14:47:15.703753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "\n",
    "# === Step 1: Load and prepare XES log ===\n",
    "print(\"Loading XES log...\")\n",
    "log = xes_importer.apply(\"/Users/6706363/Downloads/BPI_Challenge_2019.xes\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
    "\n",
    "# Drop duplicate columns, if any\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "# Rename columns to standard names\n",
    "df.rename(columns={\n",
    "    \"case:concept:name\": \"case\",\n",
    "    \"concept:name\": \"activity\",\n",
    "    \"time:timestamp\": \"timestamp\"\n",
    "}, inplace=True)\n"
   ],
   "id": "f8df5e541012010d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XES log...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6706363/PycharmProjects/PPM_NextResource/.venv/lib/python3.10/site-packages/pm4py/util/dt_parsing/parser.py:82: UserWarning: ISO8601 strings are not fully supported with strpfromiso for Python versions below 3.11\n",
      "  warnings.warn(\n",
      "/Users/6706363/PycharmProjects/PPM_NextResource/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "parsing log, completed traces :: 100%|██████████| 251734/251734 [00:47<00:00, 5270.87it/s]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-14T15:14:44.010680Z",
     "start_time": "2025-05-14T13:53:48.846109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = df.sort_values(by=[\"case\", \"timestamp\"]).reset_index(drop=True)\n",
    "df[\"next activity\"] = df.groupby(\"case\")[\"activity\"].shift(-1)\n",
    "\n",
    "for i in range(1, 7):\n",
    "    df[f\"split_{i}\"] = df[\"case\"].map(lambda x: \"train\" if hash((x, i)) % 5 < 3 else \"test\")\n",
    "\n",
    "def build_prefixes(log_df, min_length):\n",
    "    samples = []\n",
    "    for _, case_ in log_df.groupby(\"case\"):\n",
    "        while (len(case_) - 1) >= min_length:\n",
    "            pref = case_.iloc[:-1, :]\n",
    "            samples.append(list(pref[\"activity\"].values))\n",
    "            case_ = pref\n",
    "    return samples\n",
    "\n",
    "leakage_list = []\n",
    "\n",
    "print(\"Calculating example leakage only...\")\n",
    "for split in range(1, 7):\n",
    "    train = df[df[f\"split_{split}\"] == \"train\"]\n",
    "    test = df[df[f\"split_{split}\"] == \"test\"]\n",
    "\n",
    "    train_prefixes = build_prefixes(train, 1)\n",
    "    test_prefixes = build_prefixes(test, 1)\n",
    "\n",
    "    train_prefix_set = set(tuple(p) for p in train_prefixes)\n",
    "    test_prefix_set = set(tuple(p) for p in test_prefixes)\n",
    "\n",
    "    # Count how many test prefixes appear in training set\n",
    "    leaked = sum(1 for p in test_prefix_set if p in train_prefix_set)\n",
    "    leakage_percent = leaked / len(test_prefix_set) if test_prefix_set else 0\n",
    "\n",
    "    print(f\"Split {split}: Leakage = {leakage_percent:.3f}\")\n",
    "    leakage_list.append(leakage_percent)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"split\": list(range(1, 7)),\n",
    "    \"leakage\": [round(val, 3) for val in leakage_list]\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"2018_example_leakage.csv\", index=False)\n",
    "print(\"Results saved to 2018_example_leakage.csv\")"
   ],
   "id": "c35c473aaf2013e2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating example leakage only...\n",
      "Split 1: Leakage = 0.122\n",
      "Split 2: Leakage = 0.119\n",
      "Split 3: Leakage = 0.120\n",
      "Split 4: Leakage = 0.119\n",
      "Split 5: Leakage = 0.120\n",
      "Split 6: Leakage = 0.118\n",
      "Results saved to example_leakage_only.csv\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-15T14:48:51.464425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Resource-centric example leakage\n",
    "import pandas as pd\n",
    "\n",
    "df = df.sort_values(by=[\"org:resource\", \"timestamp\"]).reset_index(drop=True)\n",
    "df[\"next activity\"] = df.groupby(\"org:resource\")[\"activity\"].shift(-1)\n",
    "\n",
    "# Generate deterministic splits based on resource\n",
    "for i in range(1, 7):\n",
    "    df[f\"split_{i}\"] = df[\"org:resource\"].map(lambda x: \"train\" if hash((x, i)) % 5 < 3 else \"test\")\n",
    "\n",
    "def build_prefixes(log_df, min_length):\n",
    "    samples = []\n",
    "    for _, resource_df in log_df.groupby(\"org:resource\"):\n",
    "        while (len(resource_df) - 1) >= min_length:\n",
    "            pref = resource_df.iloc[:-1, :]\n",
    "            samples.append(list(pref[\"activity\"].values))\n",
    "            resource_df = pref\n",
    "    return samples\n",
    "\n",
    "leakage_list = []\n",
    "\n",
    "print(\"Calculating example leakage from resource perspective...\")\n",
    "for split in range(1, 7):\n",
    "    train = df[df[f\"split_{split}\"] == \"train\"]\n",
    "    test = df[df[f\"split_{split}\"] == \"test\"]\n",
    "\n",
    "    train_prefixes = build_prefixes(train, 1)\n",
    "    test_prefixes = build_prefixes(test, 1)\n",
    "\n",
    "    train_prefix_set = set(tuple(p) for p in train_prefixes)\n",
    "    test_prefix_set = set(tuple(p) for p in test_prefixes)\n",
    "\n",
    "    # Count how many test prefixes appear in training set\n",
    "    leaked = sum(1 for p in test_prefix_set if p in train_prefix_set)\n",
    "    leakage_percent = leaked / len(test_prefix_set) if test_prefix_set else 0\n",
    "\n",
    "    print(f\"Split {split}: Leakage = {leakage_percent:.3f}\")\n",
    "    leakage_list.append(leakage_percent)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"split\": list(range(1, 7)),\n",
    "    \"leakage\": [round(val, 3) for val in leakage_list]\n",
    "})\n",
    "\n",
    "results_df.to_csv(\"2019_example_leakage_resource.csv\", index=False)\n",
    "print(\"Results saved to 2019_example_leakage_resource.csv\")\n"
   ],
   "id": "313c823e6c001846",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating example leakage from resource perspective...\n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
