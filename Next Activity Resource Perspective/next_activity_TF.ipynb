{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-10T09:53:44.301946Z",
     "start_time": "2025-04-10T09:53:43.903475Z"
    }
   },
   "source": [
    "import pm4py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T09:31:25.223136Z",
     "start_time": "2025-04-11T09:29:39.177710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    event_log = pm4py.convert_to_dataframe(log)\n",
    "\n",
    "    return event_log\n",
    "\n",
    "event_log = import_xes(\"/Users/6706363/Downloads/BPI_Challenge_2019.xes\")"
   ],
   "id": "5c03dea14064b576",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 251734/251734 [00:52<00:00, 4773.62it/s]\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T14:34:25.362188Z",
     "start_time": "2025-04-11T14:34:23.366401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = event_log[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "\n",
    "df = df.sort_values(by=['org:resource', 'time:timestamp'])\n",
    "\n",
    "df.head(n=20)"
   ],
   "id": "bb3ed39dec015998",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       case:concept:name               concept:name org:resource  \\\n",
       "118143  4507004931_00010     Vendor creates invoice         NONE   \n",
       "118144  4507004931_00010  Vendor creates debit memo         NONE   \n",
       "118153  4507004931_00020     Vendor creates invoice         NONE   \n",
       "118154  4507004931_00020  Vendor creates debit memo         NONE   \n",
       "118163  4507004931_00030     Vendor creates invoice         NONE   \n",
       "118164  4507004931_00030  Vendor creates debit memo         NONE   \n",
       "118173  4507004931_00040     Vendor creates invoice         NONE   \n",
       "118174  4507004931_00040  Vendor creates debit memo         NONE   \n",
       "118183  4507004931_00050     Vendor creates invoice         NONE   \n",
       "118184  4507004931_00050  Vendor creates debit memo         NONE   \n",
       "311462  4507014062_00010     Vendor creates invoice         NONE   \n",
       "311467  4507014062_00020     Vendor creates invoice         NONE   \n",
       "311472  4507014062_00030     Vendor creates invoice         NONE   \n",
       "311477  4507014062_00040     Vendor creates invoice         NONE   \n",
       "311482  4507014062_00050     Vendor creates invoice         NONE   \n",
       "311487  4507014062_00060     Vendor creates invoice         NONE   \n",
       "311492  4507014062_00070     Vendor creates invoice         NONE   \n",
       "311497  4507014062_00080     Vendor creates invoice         NONE   \n",
       "311502  4507014062_00090     Vendor creates invoice         NONE   \n",
       "19144   4507000647_00010  Vendor creates debit memo         NONE   \n",
       "\n",
       "                  time:timestamp  \n",
       "118143 1948-01-26 22:59:00+00:00  \n",
       "118144 1948-01-26 22:59:00+00:00  \n",
       "118153 1948-01-26 22:59:00+00:00  \n",
       "118154 1948-01-26 22:59:00+00:00  \n",
       "118163 1948-01-26 22:59:00+00:00  \n",
       "118164 1948-01-26 22:59:00+00:00  \n",
       "118173 1948-01-26 22:59:00+00:00  \n",
       "118174 1948-01-26 22:59:00+00:00  \n",
       "118183 1948-01-26 22:59:00+00:00  \n",
       "118184 1948-01-26 22:59:00+00:00  \n",
       "311462 1993-05-18 21:59:00+00:00  \n",
       "311467 1993-05-18 21:59:00+00:00  \n",
       "311472 1993-05-18 21:59:00+00:00  \n",
       "311477 1993-05-18 21:59:00+00:00  \n",
       "311482 1993-05-18 21:59:00+00:00  \n",
       "311487 1993-05-18 21:59:00+00:00  \n",
       "311492 1993-05-18 21:59:00+00:00  \n",
       "311497 1993-05-18 21:59:00+00:00  \n",
       "311502 1993-05-18 21:59:00+00:00  \n",
       "19144  2001-01-23 22:59:00+00:00  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case:concept:name</th>\n",
       "      <th>concept:name</th>\n",
       "      <th>org:resource</th>\n",
       "      <th>time:timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118143</th>\n",
       "      <td>4507004931_00010</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118144</th>\n",
       "      <td>4507004931_00010</td>\n",
       "      <td>Vendor creates debit memo</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118153</th>\n",
       "      <td>4507004931_00020</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118154</th>\n",
       "      <td>4507004931_00020</td>\n",
       "      <td>Vendor creates debit memo</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118163</th>\n",
       "      <td>4507004931_00030</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118164</th>\n",
       "      <td>4507004931_00030</td>\n",
       "      <td>Vendor creates debit memo</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118173</th>\n",
       "      <td>4507004931_00040</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118174</th>\n",
       "      <td>4507004931_00040</td>\n",
       "      <td>Vendor creates debit memo</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118183</th>\n",
       "      <td>4507004931_00050</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118184</th>\n",
       "      <td>4507004931_00050</td>\n",
       "      <td>Vendor creates debit memo</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1948-01-26 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311462</th>\n",
       "      <td>4507014062_00010</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1993-05-18 21:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311467</th>\n",
       "      <td>4507014062_00020</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1993-05-18 21:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311472</th>\n",
       "      <td>4507014062_00030</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1993-05-18 21:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311477</th>\n",
       "      <td>4507014062_00040</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1993-05-18 21:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311482</th>\n",
       "      <td>4507014062_00050</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1993-05-18 21:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311487</th>\n",
       "      <td>4507014062_00060</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1993-05-18 21:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311492</th>\n",
       "      <td>4507014062_00070</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1993-05-18 21:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311497</th>\n",
       "      <td>4507014062_00080</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1993-05-18 21:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311502</th>\n",
       "      <td>4507014062_00090</td>\n",
       "      <td>Vendor creates invoice</td>\n",
       "      <td>NONE</td>\n",
       "      <td>1993-05-18 21:59:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19144</th>\n",
       "      <td>4507000647_00010</td>\n",
       "      <td>Vendor creates debit memo</td>\n",
       "      <td>NONE</td>\n",
       "      <td>2001-01-23 22:59:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T15:00:00.271332Z",
     "start_time": "2025-04-11T14:59:58.829973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "prefix_length = 700  \n",
    "\n",
    "# Function to create activity sequences\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "\n",
    "    for resource, resource_df in df.groupby('org:resource'):\n",
    "        activities = resource_df['concept:name'].values  \n",
    "        if len(activities) >= prefix_length + 1:\n",
    "            prefix = activities[:prefix_length]\n",
    "            next_activity = activities[prefix_length]\n",
    "            sequences.append(prefix)\n",
    "            next_activities.append(next_activity)\n",
    "            resources.append(resource)\n",
    "\n",
    "    sequences_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    sequences_df['next_activity'] = next_activities\n",
    "    sequences_df['org:resource'] = resources  \n",
    "\n",
    "    return sequences_df\n",
    "\n",
    "# Create sequences\n",
    "sequences_df = create_activity_sequences(df, prefix_length)"
   ],
   "id": "736894a820fc7294",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T15:00:00.507439Z",
     "start_time": "2025-04-11T15:00:00.288144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "label_encoder = LabelEncoder()\n",
    "all_activities = sequences_df[[f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']].values.flatten()\n",
    "label_encoder.fit(all_activities)\n",
    "\n",
    "# Apply encoding\n",
    "for col in [f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']:\n",
    "    sequences_df[col] = label_encoder.transform(sequences_df[col])\n",
    "\n",
    "# Store mapping\n",
    "activity_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "inverse_activity_mapping = {v: k for k, v in activity_mapping.items()}"
   ],
   "id": "a3cab09085226a06",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T11:12:36.828140Z",
     "start_time": "2025-04-11T10:48:50.656669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Experiment 1\n",
    "X = sequences_df[[f\"activity_{i+1}\" for i in range(prefix_length)]]\n",
    "y = sequences_df['next_activity']\n",
    "\n",
    "X.columns = [col.replace(\":\", \"_\") for col in X.columns]\n",
    "\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "if rare_classes:\n",
    "    if len(rare_classes) > 1:\n",
    "        # Replace multiple rare classes with the last valid label\n",
    "        new_label = len(y.unique()) - 1  \n",
    "        y = y.replace(rare_classes, new_label)\n",
    "    else:\n",
    "        # Duplicate the single rare class in both X and y\n",
    "        rare_indices = y[y.isin(rare_classes)].index  # Get indices of rare classes\n",
    "        \n",
    "        # Duplicate entries in X and y using the rare indices\n",
    "        X = pd.concat([X, X.loc[rare_indices]], ignore_index=True)  # Concatenate rows for X\n",
    "        y = pd.concat([y, y.iloc[rare_indices]], ignore_index=True)  # Concatenate labels for y\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 20\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_activities, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(ActivityTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, prefix_length, d_model))\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_activities)  # ✅ Fix: Output size matches num_activities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding  # Add positional encoding\n",
    "        x = self.transformer(x)  \n",
    "        x = x.mean(dim=1)  # Pooling\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        return x\n",
    "\n",
    "# Define Model\n",
    "num_activities = len(activity_mapping)  # ✅ Ensure correct number of activities\n",
    "model = ActivityTransformer(num_activities)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ✅ Early Stopping\n",
    "best_loss = float('inf')\n",
    "patience, patience_counter = 20, 0  # Stop if no improvement after 20 epochs\n",
    "\n",
    "# ✅ Training Loop with Early Stopping\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0  # Reset patience\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break  # Stop training\n",
    "\n",
    "# ✅ Load Best Model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# ✅ Evaluation (Accuracy, Precision, Recall, F1, SD)\n",
    "all_y_true, all_y_pred = [], []\n",
    "\n",
    "batch_acc, batch_prec, batch_rec, batch_f1 = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        batch_true = y_batch.cpu().numpy()\n",
    "        batch_pred = predicted.cpu().numpy()\n",
    "\n",
    "        # Store all predictions\n",
    "        all_y_true.extend(batch_true)\n",
    "        all_y_pred.extend(batch_pred)\n",
    "\n",
    "        # Compute metrics for the batch\n",
    "        batch_acc.append(accuracy_score(batch_true, batch_pred))\n",
    "        batch_prec.append(precision_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_rec.append(recall_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_f1.append(f1_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "# Compute Overall Metrics\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# Compute Standard Deviation (SD) for each metric\n",
    "acc_std = np.std(batch_acc)\n",
    "prec_std = np.std(batch_prec)\n",
    "rec_std = np.std(batch_rec)\n",
    "f1_std = np.std(batch_f1)\n",
    "\n",
    "# ✅ Print Metrics with SD\n",
    "print(\"\\n📊 Model Performance:\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f} (±{acc_std:.4f})\")\n",
    "print(f\"✅ Precision: {precision:.4f} (±{prec_std:.4f})\")\n",
    "print(f\"✅ Recall: {recall:.4f} (±{rec_std:.4f})\")\n",
    "print(f\"✅ F1-score: {f1:.4f} (±{f1_std:.4f})\")\n"
   ],
   "id": "6591c0b4fe5a52f9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6706363/PycharmProjects/PPM_NextResource/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7706\n",
      "Epoch 2, Loss: 0.8308\n",
      "Epoch 3, Loss: 0.6547\n",
      "Epoch 4, Loss: 0.6236\n",
      "Epoch 5, Loss: 0.5113\n",
      "Epoch 6, Loss: 0.4686\n",
      "Epoch 7, Loss: 0.4556\n",
      "Epoch 8, Loss: 0.4343\n",
      "Epoch 9, Loss: 0.3968\n",
      "Epoch 10, Loss: 0.3979\n",
      "Epoch 11, Loss: 0.3960\n",
      "Epoch 12, Loss: 0.3973\n",
      "Epoch 13, Loss: 0.3869\n",
      "Epoch 14, Loss: 0.3898\n",
      "Epoch 15, Loss: 0.3874\n",
      "Epoch 16, Loss: 0.3676\n",
      "Epoch 17, Loss: 0.3729\n",
      "Epoch 18, Loss: 0.3823\n",
      "Epoch 19, Loss: 0.3802\n",
      "Epoch 20, Loss: 0.3588\n",
      "Epoch 21, Loss: 0.3732\n",
      "Epoch 22, Loss: 0.3589\n",
      "Epoch 23, Loss: 0.3632\n",
      "Epoch 24, Loss: 0.4212\n",
      "Epoch 25, Loss: 0.3634\n",
      "Epoch 26, Loss: 0.3509\n",
      "Epoch 27, Loss: 0.3573\n",
      "Epoch 28, Loss: 0.3493\n",
      "Epoch 29, Loss: 0.3692\n",
      "Epoch 30, Loss: 0.3509\n",
      "Epoch 31, Loss: 0.3559\n",
      "Epoch 32, Loss: 0.3553\n",
      "Epoch 33, Loss: 0.3534\n",
      "Epoch 34, Loss: 0.3450\n",
      "Epoch 35, Loss: 0.3456\n",
      "Epoch 36, Loss: 0.3260\n",
      "Epoch 37, Loss: 0.3285\n",
      "Epoch 38, Loss: 0.3329\n",
      "Epoch 39, Loss: 0.3340\n",
      "Epoch 40, Loss: 0.3507\n",
      "Epoch 41, Loss: 0.3426\n",
      "Epoch 42, Loss: 0.3375\n",
      "Epoch 43, Loss: 0.3302\n",
      "Epoch 44, Loss: 0.3287\n",
      "Epoch 45, Loss: 0.3291\n",
      "Epoch 46, Loss: 0.3793\n",
      "Epoch 47, Loss: 0.3388\n",
      "Epoch 48, Loss: 0.3269\n",
      "Epoch 49, Loss: 0.3223\n",
      "Epoch 50, Loss: 0.3289\n",
      "Epoch 51, Loss: 0.3430\n",
      "Epoch 52, Loss: 0.3167\n",
      "Epoch 53, Loss: 0.3258\n",
      "Epoch 54, Loss: 0.3050\n",
      "Epoch 55, Loss: 0.3007\n",
      "Epoch 56, Loss: 0.3235\n",
      "Epoch 57, Loss: 0.3072\n",
      "Epoch 58, Loss: 0.3119\n",
      "Epoch 59, Loss: 0.3062\n",
      "Epoch 60, Loss: 0.3045\n",
      "Epoch 61, Loss: 0.3003\n",
      "Epoch 62, Loss: 0.3037\n",
      "Epoch 63, Loss: 0.3080\n",
      "Epoch 64, Loss: 0.2938\n",
      "Epoch 65, Loss: 0.2933\n",
      "Epoch 66, Loss: 0.2838\n",
      "Epoch 67, Loss: 0.2817\n",
      "Epoch 68, Loss: 0.2855\n",
      "Epoch 69, Loss: 0.2819\n",
      "Epoch 70, Loss: 0.2784\n",
      "Epoch 71, Loss: 0.2738\n",
      "Epoch 72, Loss: 0.2610\n",
      "Epoch 73, Loss: 0.2679\n",
      "Epoch 74, Loss: 0.2716\n",
      "Epoch 75, Loss: 0.2815\n",
      "Epoch 76, Loss: 0.2775\n",
      "Epoch 77, Loss: 0.2939\n",
      "Epoch 78, Loss: 0.2697\n",
      "Epoch 79, Loss: 0.2760\n",
      "Epoch 80, Loss: 0.2704\n",
      "Epoch 81, Loss: 0.2727\n",
      "Epoch 82, Loss: 0.2643\n",
      "Epoch 83, Loss: 0.2702\n",
      "Epoch 84, Loss: 0.2646\n",
      "Epoch 85, Loss: 0.2483\n",
      "Epoch 86, Loss: 0.2573\n",
      "Epoch 87, Loss: 0.2513\n",
      "Epoch 88, Loss: 0.2384\n",
      "Epoch 89, Loss: 0.2544\n",
      "Epoch 90, Loss: 0.2501\n",
      "Epoch 91, Loss: 0.2604\n",
      "Epoch 92, Loss: 0.2681\n",
      "Epoch 93, Loss: 0.2494\n",
      "Epoch 94, Loss: 0.2331\n",
      "Epoch 95, Loss: 0.2316\n",
      "Epoch 96, Loss: 0.2378\n",
      "Epoch 97, Loss: 0.2273\n",
      "Epoch 98, Loss: 0.2283\n",
      "Epoch 99, Loss: 0.2201\n",
      "Epoch 100, Loss: 0.2240\n",
      "Epoch 101, Loss: 0.2198\n",
      "Epoch 102, Loss: 0.2287\n",
      "Epoch 103, Loss: 0.2173\n",
      "Epoch 104, Loss: 0.2103\n",
      "Epoch 105, Loss: 0.2325\n",
      "Epoch 106, Loss: 0.2167\n",
      "Epoch 107, Loss: 0.2160\n",
      "Epoch 108, Loss: 0.2129\n",
      "Epoch 109, Loss: 0.2160\n",
      "Epoch 110, Loss: 0.2163\n",
      "Epoch 111, Loss: 0.2059\n",
      "Epoch 112, Loss: 0.2026\n",
      "Epoch 113, Loss: 0.1938\n",
      "Epoch 114, Loss: 0.2108\n",
      "Epoch 115, Loss: 0.2062\n",
      "Epoch 116, Loss: 0.2031\n",
      "Epoch 117, Loss: 0.1974\n",
      "Epoch 118, Loss: 0.1905\n",
      "Epoch 119, Loss: 0.1889\n",
      "Epoch 120, Loss: 0.1880\n",
      "Epoch 121, Loss: 0.1924\n",
      "Epoch 122, Loss: 0.1724\n",
      "Epoch 123, Loss: 0.1748\n",
      "Epoch 124, Loss: 0.1692\n",
      "Epoch 125, Loss: 0.1604\n",
      "Epoch 126, Loss: 0.1727\n",
      "Epoch 127, Loss: 0.1660\n",
      "Epoch 128, Loss: 0.1563\n",
      "Epoch 129, Loss: 0.1590\n",
      "Epoch 130, Loss: 0.1624\n",
      "Epoch 131, Loss: 0.1711\n",
      "Epoch 132, Loss: 0.1561\n",
      "Epoch 133, Loss: 0.1625\n",
      "Epoch 134, Loss: 0.1533\n",
      "Epoch 135, Loss: 0.1512\n",
      "Epoch 136, Loss: 0.1583\n",
      "Epoch 137, Loss: 0.1428\n",
      "Epoch 138, Loss: 0.1599\n",
      "Epoch 139, Loss: 0.1647\n",
      "Epoch 140, Loss: 0.1507\n",
      "Epoch 141, Loss: 0.1499\n",
      "Epoch 142, Loss: 0.1419\n",
      "Epoch 143, Loss: 0.1397\n",
      "Epoch 144, Loss: 0.1351\n",
      "Epoch 145, Loss: 0.1404\n",
      "Epoch 146, Loss: 0.1320\n",
      "Epoch 147, Loss: 0.1344\n",
      "Epoch 148, Loss: 0.1450\n",
      "Epoch 149, Loss: 0.1349\n",
      "Epoch 150, Loss: 0.1580\n",
      "Epoch 151, Loss: 0.1608\n",
      "Epoch 152, Loss: 0.1463\n",
      "Epoch 153, Loss: 0.1361\n",
      "Epoch 154, Loss: 0.1271\n",
      "Epoch 155, Loss: 0.1255\n",
      "Epoch 156, Loss: 0.1228\n",
      "Epoch 157, Loss: 0.1224\n",
      "Epoch 158, Loss: 0.1111\n",
      "Epoch 159, Loss: 0.1191\n",
      "Epoch 160, Loss: 0.1268\n",
      "Epoch 161, Loss: 0.1166\n",
      "Epoch 162, Loss: 0.1096\n",
      "Epoch 163, Loss: 0.1089\n",
      "Epoch 164, Loss: 0.1122\n",
      "Epoch 165, Loss: 0.1028\n",
      "Epoch 166, Loss: 0.0989\n",
      "Epoch 167, Loss: 0.1034\n",
      "Epoch 168, Loss: 0.0954\n",
      "Epoch 169, Loss: 0.0962\n",
      "Epoch 170, Loss: 0.1065\n",
      "Epoch 171, Loss: 0.1053\n",
      "Epoch 172, Loss: 0.0912\n",
      "Epoch 173, Loss: 0.0933\n",
      "Epoch 174, Loss: 0.0950\n",
      "Epoch 175, Loss: 0.0974\n",
      "Epoch 176, Loss: 0.0879\n",
      "Epoch 177, Loss: 0.1008\n",
      "Epoch 178, Loss: 0.0912\n",
      "Epoch 179, Loss: 0.0888\n",
      "Epoch 180, Loss: 0.0911\n",
      "Epoch 181, Loss: 0.0860\n",
      "Epoch 182, Loss: 0.0835\n",
      "Epoch 183, Loss: 0.0808\n",
      "Epoch 184, Loss: 0.0793\n",
      "Epoch 185, Loss: 0.0807\n",
      "Epoch 186, Loss: 0.0799\n",
      "Epoch 187, Loss: 0.0733\n",
      "Epoch 188, Loss: 0.0804\n",
      "Epoch 189, Loss: 0.0659\n",
      "Epoch 190, Loss: 0.0726\n",
      "Epoch 191, Loss: 0.0716\n",
      "Epoch 192, Loss: 0.0717\n",
      "Epoch 193, Loss: 0.0708\n",
      "Epoch 194, Loss: 0.0653\n",
      "Epoch 195, Loss: 0.0672\n",
      "Epoch 196, Loss: 0.0670\n",
      "Epoch 197, Loss: 0.0665\n",
      "Epoch 198, Loss: 0.0563\n",
      "Epoch 199, Loss: 0.0617\n",
      "Epoch 200, Loss: 0.0641\n",
      "\n",
      "📊 Model Performance:\n",
      "✅ Accuracy: 0.8857 (±0.0167)\n",
      "✅ Precision: 0.8469 (±0.0392)\n",
      "✅ Recall: 0.8857 (±0.0167)\n",
      "✅ F1-score: 0.8575 (±0.0317)\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T13:42:49.859881Z",
     "start_time": "2025-04-11T13:42:49.353865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Experiment 2: Next Activity Prediction with activity information\n",
    "import binary_classifier\n",
    "\n",
    "ra_diversity_matrix = binary_classifier.create_diversity_matrix(event_log)\n",
    "ra_diversity_matrix_binary = ra_diversity_matrix.copy()\n",
    "# Apply a binary transformation: any count > 0 becomes 1 (yes), else 0 (no)\n",
    "ra_diversity_matrix_binary.iloc[:, 1:] = (ra_diversity_matrix_binary.iloc[:, 1:] > 0).astype(int)\n",
    "\n",
    "activities = ra_diversity_matrix.columns[1:].tolist()  # Convert to a list of activities\n",
    "print(activities)\n",
    "binary_activities = ra_diversity_matrix_binary.iloc[:, :]\n"
   ],
   "id": "1cd1cfa897cd70ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Block Purchase Order Item', 'Cancel Goods Receipt', 'Cancel Invoice Receipt', 'Cancel Subsequent Invoice', 'Change Approval for Purchase Order', 'Change Currency', 'Change Delivery Indicator', 'Change Final Invoice Indicator', 'Change Price', 'Change Quantity', 'Change Rejection Indicator', 'Change Storage Location', 'Change payment term', 'Clear Invoice', 'Create Purchase Order Item', 'Create Purchase Requisition Item', 'Delete Purchase Order Item', 'Reactivate Purchase Order Item', 'Receive Order Confirmation', 'Record Goods Receipt', 'Record Invoice Receipt', 'Record Service Entry Sheet', 'Record Subsequent Invoice', 'Release Purchase Order', 'Release Purchase Requisition', 'Remove Payment Block', 'SRM: Awaiting Approval', 'SRM: Change was Transmitted', 'SRM: Complete', 'SRM: Created', 'SRM: Deleted', 'SRM: Document Completed', 'SRM: Held', 'SRM: In Transfer to Execution Syst.', 'SRM: Incomplete', 'SRM: Ordered', 'SRM: Transaction Completed', 'SRM: Transfer Failed (E.Sys.)', 'Set Payment Block', 'Update Order Confirmation', 'Vendor creates debit memo', 'Vendor creates invoice']\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T13:42:49.967318Z",
     "start_time": "2025-04-11T13:42:49.953062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Keep only resources that are in sequences_df\n",
    "filtered_binary_activities = binary_activities[binary_activities['org:resource'].isin(sequences_df['org:resource'])]\n",
    "\n",
    "# Reset index to ensure proper alignment\n",
    "filtered_binary_activities = filtered_binary_activities.reset_index(drop=True)\n",
    "sequences_df = sequences_df.reset_index(drop=True)\n",
    "\n",
    "# Merge again\n",
    "merged_df = pd.concat([sequences_df, filtered_binary_activities], axis=1)"
   ],
   "id": "e22a8c8edbe37b2d",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T14:03:22.734383Z",
     "start_time": "2025-04-11T13:42:50.845498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "X = merged_df[[f\"activity_{i+1}\" for i in range(prefix_length)] + activities]\n",
    "y = merged_df['next_activity']\n",
    "\n",
    "X.columns = [col.replace(\":\", \"_\") for col in X.columns]\n",
    "\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "if rare_classes:\n",
    "    if len(rare_classes) > 1:\n",
    "        # Replace multiple rare classes with the last valid label\n",
    "        new_label = len(y.unique()) - 1  \n",
    "        y = y.replace(rare_classes, new_label)\n",
    "    else:\n",
    "        # Duplicate the single rare class in both X and y\n",
    "        rare_indices = y[y.isin(rare_classes)].index  # Get indices of rare classes\n",
    "        \n",
    "        # Duplicate entries in X and y using the rare indices\n",
    "        X = pd.concat([X, X.loc[rare_indices]], ignore_index=True)  # Concatenate rows for X\n",
    "        y = pd.concat([y, y.iloc[rare_indices]], ignore_index=True)  # Concatenate labels for y\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 20\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_activities, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(ActivityTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, prefix_length + len(activities), d_model))\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_activities)  # ✅ Fix: Output size matches num_activities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding  # Add positional encoding\n",
    "        x = self.transformer(x)  \n",
    "        x = x.mean(dim=1)  # Pooling\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        return x\n",
    "\n",
    "# Define Model\n",
    "num_activities = len(activity_mapping)  # ✅ Ensure correct number of activities\n",
    "model = ActivityTransformer(num_activities)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ✅ Early Stopping\n",
    "best_loss = float('inf')\n",
    "patience, patience_counter = 20, 0  # Stop if no improvement after 20 epochs\n",
    "\n",
    "# ✅ Training Loop with Early Stopping\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0  # Reset patience\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break  # Stop training\n",
    "\n",
    "# ✅ Load Best Model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# ✅ Evaluation (Accuracy, Precision, Recall, F1, SD)\n",
    "all_y_true, all_y_pred = [], []\n",
    "\n",
    "batch_acc, batch_prec, batch_rec, batch_f1 = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        batch_true = y_batch.cpu().numpy()\n",
    "        batch_pred = predicted.cpu().numpy()\n",
    "\n",
    "        # Store all predictions\n",
    "        all_y_true.extend(batch_true)\n",
    "        all_y_pred.extend(batch_pred)\n",
    "\n",
    "        # Compute metrics for the batch\n",
    "        batch_acc.append(accuracy_score(batch_true, batch_pred))\n",
    "        batch_prec.append(precision_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_rec.append(recall_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_f1.append(f1_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "# Compute Overall Metrics\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# Compute Standard Deviation (SD) for each metric\n",
    "acc_std = np.std(batch_acc)\n",
    "prec_std = np.std(batch_prec)\n",
    "rec_std = np.std(batch_rec)\n",
    "f1_std = np.std(batch_f1)\n",
    "\n",
    "# ✅ Print Metrics with SD\n",
    "print(\"\\n📊 Model Performance:\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f} (±{acc_std:.4f})\")\n",
    "print(f\"✅ Precision: {precision:.4f} (±{prec_std:.4f})\")\n",
    "print(f\"✅ Recall: {recall:.4f} (±{rec_std:.4f})\")\n",
    "print(f\"✅ F1-score: {f1:.4f} (±{f1_std:.4f})\")\n"
   ],
   "id": "9b62f14cafdfd47b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6706363/PycharmProjects/PPM_NextResource/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5453\n",
      "Epoch 2, Loss: 0.7273\n",
      "Epoch 3, Loss: 0.6359\n",
      "Epoch 4, Loss: 0.5269\n",
      "Epoch 5, Loss: 0.5098\n",
      "Epoch 6, Loss: 0.4727\n",
      "Epoch 7, Loss: 0.4638\n",
      "Epoch 8, Loss: 0.4247\n",
      "Epoch 9, Loss: 0.4339\n",
      "Epoch 10, Loss: 0.4153\n",
      "Epoch 11, Loss: 0.4141\n",
      "Epoch 12, Loss: 0.3834\n",
      "Epoch 13, Loss: 0.3914\n",
      "Epoch 14, Loss: 0.4090\n",
      "Epoch 15, Loss: 0.4058\n",
      "Epoch 16, Loss: 0.3855\n",
      "Epoch 17, Loss: 0.4040\n",
      "Epoch 18, Loss: 0.3676\n",
      "Epoch 19, Loss: 0.3871\n",
      "Epoch 20, Loss: 0.3638\n",
      "Epoch 21, Loss: 0.3795\n",
      "Epoch 22, Loss: 0.3696\n",
      "Epoch 23, Loss: 0.3647\n",
      "Epoch 24, Loss: 0.3583\n",
      "Epoch 25, Loss: 0.3460\n",
      "Epoch 26, Loss: 0.3541\n",
      "Epoch 27, Loss: 0.3597\n",
      "Epoch 28, Loss: 0.3523\n",
      "Epoch 29, Loss: 0.3449\n",
      "Epoch 30, Loss: 0.3358\n",
      "Epoch 31, Loss: 0.3494\n",
      "Epoch 32, Loss: 0.3415\n",
      "Epoch 33, Loss: 0.3466\n",
      "Epoch 34, Loss: 0.3617\n",
      "Epoch 35, Loss: 0.3376\n",
      "Epoch 36, Loss: 0.3514\n",
      "Epoch 37, Loss: 0.3379\n",
      "Epoch 38, Loss: 0.3415\n",
      "Epoch 39, Loss: 0.3541\n",
      "Epoch 40, Loss: 0.3282\n",
      "Epoch 41, Loss: 0.3273\n",
      "Epoch 42, Loss: 0.3301\n",
      "Epoch 43, Loss: 0.3216\n",
      "Epoch 44, Loss: 0.3215\n",
      "Epoch 45, Loss: 0.3238\n",
      "Epoch 46, Loss: 0.3131\n",
      "Epoch 47, Loss: 0.3169\n",
      "Epoch 48, Loss: 0.3210\n",
      "Epoch 49, Loss: 0.3202\n",
      "Epoch 50, Loss: 0.3195\n",
      "Epoch 51, Loss: 0.3107\n",
      "Epoch 52, Loss: 0.3176\n",
      "Epoch 53, Loss: 0.3211\n",
      "Epoch 54, Loss: 0.3094\n",
      "Epoch 55, Loss: 0.3053\n",
      "Epoch 56, Loss: 0.3102\n",
      "Epoch 57, Loss: 0.2926\n",
      "Epoch 58, Loss: 0.2898\n",
      "Epoch 59, Loss: 0.2907\n",
      "Epoch 60, Loss: 0.2863\n",
      "Epoch 61, Loss: 0.2827\n",
      "Epoch 62, Loss: 0.2910\n",
      "Epoch 63, Loss: 0.2920\n",
      "Epoch 64, Loss: 0.2908\n",
      "Epoch 65, Loss: 0.2854\n",
      "Epoch 66, Loss: 0.2835\n",
      "Epoch 67, Loss: 0.2792\n",
      "Epoch 68, Loss: 0.2736\n",
      "Epoch 69, Loss: 0.2930\n",
      "Epoch 70, Loss: 0.2719\n",
      "Epoch 71, Loss: 0.2825\n",
      "Epoch 72, Loss: 0.2763\n",
      "Epoch 73, Loss: 0.2575\n",
      "Epoch 74, Loss: 0.2863\n",
      "Epoch 75, Loss: 0.2645\n",
      "Epoch 76, Loss: 0.2602\n",
      "Epoch 77, Loss: 0.2528\n",
      "Epoch 78, Loss: 0.2610\n",
      "Epoch 79, Loss: 0.2429\n",
      "Epoch 80, Loss: 0.2434\n",
      "Epoch 81, Loss: 0.2420\n",
      "Epoch 82, Loss: 0.2490\n",
      "Epoch 83, Loss: 0.2595\n",
      "Epoch 84, Loss: 0.2623\n",
      "Epoch 85, Loss: 0.2446\n",
      "Epoch 86, Loss: 0.2365\n",
      "Epoch 87, Loss: 0.2288\n",
      "Epoch 88, Loss: 0.2231\n",
      "Epoch 89, Loss: 0.2289\n",
      "Epoch 90, Loss: 0.2271\n",
      "Epoch 91, Loss: 0.2269\n",
      "Epoch 92, Loss: 0.2228\n",
      "Epoch 93, Loss: 0.2194\n",
      "Epoch 94, Loss: 0.2194\n",
      "Epoch 95, Loss: 0.2100\n",
      "Epoch 96, Loss: 0.2096\n",
      "Epoch 97, Loss: 0.2109\n",
      "Epoch 98, Loss: 0.2084\n",
      "Epoch 99, Loss: 0.2052\n",
      "Epoch 100, Loss: 0.2086\n",
      "Epoch 101, Loss: 0.2079\n",
      "Epoch 102, Loss: 0.2004\n",
      "Epoch 103, Loss: 0.2059\n",
      "Epoch 104, Loss: 0.2175\n",
      "Epoch 105, Loss: 0.2049\n",
      "Epoch 106, Loss: 0.2007\n",
      "Epoch 107, Loss: 0.1949\n",
      "Epoch 108, Loss: 0.1935\n",
      "Epoch 109, Loss: 0.1969\n",
      "Epoch 110, Loss: 0.2169\n",
      "Epoch 111, Loss: 0.2165\n",
      "Epoch 112, Loss: 0.2102\n",
      "Epoch 113, Loss: 0.1954\n",
      "Epoch 114, Loss: 0.1766\n",
      "Epoch 115, Loss: 0.1802\n",
      "Epoch 116, Loss: 0.1702\n",
      "Epoch 117, Loss: 0.1761\n",
      "Epoch 118, Loss: 0.1758\n",
      "Epoch 119, Loss: 0.1747\n",
      "Epoch 120, Loss: 0.1783\n",
      "Epoch 121, Loss: 0.1818\n",
      "Epoch 122, Loss: 0.1953\n",
      "Epoch 123, Loss: 0.1827\n",
      "Epoch 124, Loss: 0.1750\n",
      "Epoch 125, Loss: 0.1710\n",
      "Epoch 126, Loss: 0.1770\n",
      "Epoch 127, Loss: 0.1842\n",
      "Epoch 128, Loss: 0.1581\n",
      "Epoch 129, Loss: 0.1607\n",
      "Epoch 130, Loss: 0.1579\n",
      "Epoch 131, Loss: 0.1474\n",
      "Epoch 132, Loss: 0.1479\n",
      "Epoch 133, Loss: 0.1363\n",
      "Epoch 134, Loss: 0.1338\n",
      "Epoch 135, Loss: 0.1625\n",
      "Epoch 136, Loss: 0.1417\n",
      "Epoch 137, Loss: 0.1409\n",
      "Epoch 138, Loss: 0.1389\n",
      "Epoch 139, Loss: 0.1350\n",
      "Epoch 140, Loss: 0.1313\n",
      "Epoch 141, Loss: 0.1319\n",
      "Epoch 142, Loss: 0.1216\n",
      "Epoch 143, Loss: 0.1177\n",
      "Epoch 144, Loss: 0.1272\n",
      "Epoch 145, Loss: 0.1261\n",
      "Epoch 146, Loss: 0.1319\n",
      "Epoch 147, Loss: 0.1966\n",
      "Epoch 148, Loss: 0.2095\n",
      "Epoch 149, Loss: 0.2724\n",
      "Epoch 150, Loss: 0.1931\n",
      "Epoch 151, Loss: 0.2440\n",
      "Epoch 152, Loss: 0.2156\n",
      "Epoch 153, Loss: 0.3721\n",
      "Epoch 154, Loss: 0.3117\n",
      "Epoch 155, Loss: 0.2211\n",
      "Epoch 156, Loss: 0.2940\n",
      "Epoch 157, Loss: 0.3294\n",
      "Epoch 158, Loss: 0.3198\n",
      "Epoch 159, Loss: 0.2246\n",
      "Epoch 160, Loss: 0.2126\n",
      "Epoch 161, Loss: 0.1724\n",
      "Epoch 162, Loss: 0.1793\n",
      "Epoch 163, Loss: 0.1561\n",
      "Early stopping at epoch 163\n",
      "\n",
      "📊 Model Performance:\n",
      "✅ Accuracy: 0.8857 (±0.0167)\n",
      "✅ Precision: 0.8469 (±0.0392)\n",
      "✅ Recall: 0.8857 (±0.0167)\n",
      "✅ F1-score: 0.8575 (±0.0317)\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T15:00:07.522233Z",
     "start_time": "2025-04-11T15:00:07.508923Z"
    }
   },
   "cell_type": "code",
   "source": "sequences_df = sequences_df.drop(columns=['org:resource'])",
   "id": "55c96e80ecffa90b",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T14:50:58.462716Z",
     "start_time": "2025-04-11T14:50:11.321367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Get unique activities from the dataset\n",
    "unique_activities = sorted(set(sequences_df.values.flatten()))\n",
    "\n",
    "# Generate all possible transitions\n",
    "all_possible_transitions = {(a, b) for a in unique_activities for b in unique_activities}\n",
    "\n",
    "# Create a list to store transition count dictionaries\n",
    "transition_counts = []\n",
    "\n",
    "# Iterate through each row to count transitions\n",
    "for _, row in sequences_df.iterrows():\n",
    "    transitions = defaultdict(int)\n",
    "    activities = row.dropna().values  # Extract non-null activities\n",
    "\n",
    "    # Count actual transitions\n",
    "    for i in range(len(activities) - 1):\n",
    "        transition = (activities[i], activities[i + 1])\n",
    "        transitions[transition] += 1\n",
    "\n",
    "    # Ensure every possible transition exists (fill with 0 if not present)\n",
    "    row_counts = {t: transitions.get(t, 0) for t in all_possible_transitions}\n",
    "    transition_counts.append(row_counts)\n",
    "\n",
    "# Convert list of transition count dictionaries to a DataFrame\n",
    "transitions_df = pd.DataFrame(transition_counts)\n",
    "\n",
    "# Rename columns to string format (e.g., '0->0', '0->1', etc.)\n",
    "transitions_df.columns = [f\"{a}->{b}\" for a, b in transitions_df.columns]\n",
    "\n",
    "# Merge with original DataFrame\n",
    "result_df = pd.concat([sequences_df, transitions_df], axis=1)\n",
    "\n",
    "X = result_df.drop(columns=['next_activity'])\n",
    "y = result_df['next_activity']\n",
    "\n",
    "X.columns = [col.replace(\":\", \"_\") for col in X.columns]\n",
    "\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "if rare_classes:\n",
    "    if len(rare_classes) > 1:\n",
    "        # Replace multiple rare classes with the last valid label\n",
    "        new_label = len(y.unique()) - 1  \n",
    "        y = y.replace(rare_classes, new_label)\n",
    "    else:\n",
    "        # Duplicate the single rare class in both X and y\n",
    "        rare_indices = y[y.isin(rare_classes)].index  # Get indices of rare classes\n",
    "        \n",
    "        # Duplicate entries in X and y using the rare indices\n",
    "        X = pd.concat([X, X.loc[rare_indices]], ignore_index=True)  # Concatenate rows for X\n",
    "        y = pd.concat([y, y.iloc[rare_indices]], ignore_index=True)  # Concatenate labels for y\n",
    "\n",
    "# Feature selection (now after handling rare classes)\n",
    "X_selected = SelectKBest(mutual_info_classif, k=20).fit_transform(X, y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_selected, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Min in X_train: {X_train.min()}, Max in X_train: {X_train.max()}\")\n",
    "print(f\"Min in X_test: {X_test.min()}, Max in X_test: {X_test.max()}\")\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 20\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_activities, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(ActivityTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, X_batch.size(1), d_model))  # Use X_batch.size(1)\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_activities)  # ✅ Fix: Output size matches num_activities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding  # Add positional encoding\n",
    "        x = self.transformer(x)  \n",
    "        x = x.mean(dim=1)  # Pooling\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        return x\n",
    "\n",
    "# Define Model\n",
    "num_activities = X_tensor.max().item() + 1  \n",
    "model = ActivityTransformer(num_activities)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# ✅ Early Stopping\n",
    "best_loss = float('inf')\n",
    "patience, patience_counter = 20, 0  # Stop if no improvement after 20 epochs\n",
    "\n",
    "# ✅ Training Loop with Early Stopping\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0  # Reset patience\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break  # Stop training\n",
    "\n",
    "# ✅ Load Best Model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# ✅ Evaluation (Accuracy, Precision, Recall, F1, SD)\n",
    "all_y_true, all_y_pred = [], []\n",
    "\n",
    "batch_acc, batch_prec, batch_rec, batch_f1 = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        batch_true = y_batch.cpu().numpy()\n",
    "        batch_pred = predicted.cpu().numpy()\n",
    "\n",
    "        # Store all predictions\n",
    "        all_y_true.extend(batch_true)\n",
    "        all_y_pred.extend(batch_pred)\n",
    "\n",
    "        # Compute metrics for the batch\n",
    "        batch_acc.append(accuracy_score(batch_true, batch_pred))\n",
    "        batch_prec.append(precision_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_rec.append(recall_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_f1.append(f1_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "# Compute Overall Metrics\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# Compute Standard Deviation (SD) for each metric\n",
    "acc_std = np.std(batch_acc)\n",
    "prec_std = np.std(batch_prec)\n",
    "rec_std = np.std(batch_rec)\n",
    "f1_std = np.std(batch_f1)\n",
    "\n",
    "# ✅ Print Metrics with SD\n",
    "print(\"\\n📊 Model Performance:\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f} (±{acc_std:.4f})\")\n",
    "print(f\"✅ Precision: {precision:.4f} (±{prec_std:.4f})\")\n",
    "print(f\"✅ Recall: {recall:.4f} (±{rec_std:.4f})\")\n",
    "print(f\"✅ F1-score: {f1:.4f} (±{f1_std:.4f})\")"
   ],
   "id": "1abde873183c6032",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min in X_train: 1, Max in X_train: 31\n",
      "Min in X_test: 1, Max in X_test: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6706363/PycharmProjects/PPM_NextResource/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4990\n",
      "Epoch 2, Loss: 0.6353\n",
      "Epoch 3, Loss: 0.4478\n",
      "Epoch 4, Loss: 0.3649\n",
      "Epoch 5, Loss: 0.3027\n",
      "Epoch 6, Loss: 0.2720\n",
      "Epoch 7, Loss: 0.2622\n",
      "Epoch 8, Loss: 0.2681\n",
      "Epoch 9, Loss: 0.2435\n",
      "Epoch 10, Loss: 0.2452\n",
      "Epoch 11, Loss: 0.2320\n",
      "Epoch 12, Loss: 0.2205\n",
      "Epoch 13, Loss: 0.2088\n",
      "Epoch 14, Loss: 0.2110\n",
      "Epoch 15, Loss: 0.2020\n",
      "Epoch 16, Loss: 0.2083\n",
      "Epoch 17, Loss: 0.1868\n",
      "Epoch 18, Loss: 0.2026\n",
      "Epoch 19, Loss: 0.1853\n",
      "Epoch 20, Loss: 0.1897\n",
      "Epoch 21, Loss: 0.1837\n",
      "Epoch 22, Loss: 0.1860\n",
      "Epoch 23, Loss: 0.1780\n",
      "Epoch 24, Loss: 0.1806\n",
      "Epoch 25, Loss: 0.1812\n",
      "Epoch 26, Loss: 0.1689\n",
      "Epoch 27, Loss: 0.1588\n",
      "Epoch 28, Loss: 0.1596\n",
      "Epoch 29, Loss: 0.1954\n",
      "Epoch 30, Loss: 0.1663\n",
      "Epoch 31, Loss: 0.1651\n",
      "Epoch 32, Loss: 0.1538\n",
      "Epoch 33, Loss: 0.1603\n",
      "Epoch 34, Loss: 0.1453\n",
      "Epoch 35, Loss: 0.1522\n",
      "Epoch 36, Loss: 0.1463\n",
      "Epoch 37, Loss: 0.1427\n",
      "Epoch 38, Loss: 0.1383\n",
      "Epoch 39, Loss: 0.1445\n",
      "Epoch 40, Loss: 0.1528\n",
      "Epoch 41, Loss: 0.1299\n",
      "Epoch 42, Loss: 0.1305\n",
      "Epoch 43, Loss: 0.1257\n",
      "Epoch 44, Loss: 0.1301\n",
      "Epoch 45, Loss: 0.1375\n",
      "Epoch 46, Loss: 0.1314\n",
      "Epoch 47, Loss: 0.1240\n",
      "Epoch 48, Loss: 0.1205\n",
      "Epoch 49, Loss: 0.1240\n",
      "Epoch 50, Loss: 0.1342\n",
      "Epoch 51, Loss: 0.1401\n",
      "Epoch 52, Loss: 0.1357\n",
      "Epoch 53, Loss: 0.1436\n",
      "Epoch 54, Loss: 0.1237\n",
      "Epoch 55, Loss: 0.1185\n",
      "Epoch 56, Loss: 0.1246\n",
      "Epoch 57, Loss: 0.1036\n",
      "Epoch 58, Loss: 0.0981\n",
      "Epoch 59, Loss: 0.0999\n",
      "Epoch 60, Loss: 0.1003\n",
      "Epoch 61, Loss: 0.0942\n",
      "Epoch 62, Loss: 0.0952\n",
      "Epoch 63, Loss: 0.0924\n",
      "Epoch 64, Loss: 0.0923\n",
      "Epoch 65, Loss: 0.0930\n",
      "Epoch 66, Loss: 0.0884\n",
      "Epoch 67, Loss: 0.0930\n",
      "Epoch 68, Loss: 0.0990\n",
      "Epoch 69, Loss: 0.0908\n",
      "Epoch 70, Loss: 0.0831\n",
      "Epoch 71, Loss: 0.0878\n",
      "Epoch 72, Loss: 0.0821\n",
      "Epoch 73, Loss: 0.0864\n",
      "Epoch 74, Loss: 0.0819\n",
      "Epoch 75, Loss: 0.0821\n",
      "Epoch 76, Loss: 0.0858\n",
      "Epoch 77, Loss: 0.0811\n",
      "Epoch 78, Loss: 0.0837\n",
      "Epoch 79, Loss: 0.0792\n",
      "Epoch 80, Loss: 0.0809\n",
      "Epoch 81, Loss: 0.0941\n",
      "Epoch 82, Loss: 0.0725\n",
      "Epoch 83, Loss: 0.0780\n",
      "Epoch 84, Loss: 0.0735\n",
      "Epoch 85, Loss: 0.0745\n",
      "Epoch 86, Loss: 0.0752\n",
      "Epoch 87, Loss: 0.0725\n",
      "Epoch 88, Loss: 0.0757\n",
      "Epoch 89, Loss: 0.0726\n",
      "Epoch 90, Loss: 0.0721\n",
      "Epoch 91, Loss: 0.0666\n",
      "Epoch 92, Loss: 0.0659\n",
      "Epoch 93, Loss: 0.0660\n",
      "Epoch 94, Loss: 0.0732\n",
      "Epoch 95, Loss: 0.0654\n",
      "Epoch 96, Loss: 0.0738\n",
      "Epoch 97, Loss: 0.0679\n",
      "Epoch 98, Loss: 0.0651\n",
      "Epoch 99, Loss: 0.0660\n",
      "Epoch 100, Loss: 0.0641\n",
      "Epoch 101, Loss: 0.0660\n",
      "Epoch 102, Loss: 0.0616\n",
      "Epoch 103, Loss: 0.0597\n",
      "Epoch 104, Loss: 0.0640\n",
      "Epoch 105, Loss: 0.0615\n",
      "Epoch 106, Loss: 0.0598\n",
      "Epoch 107, Loss: 0.0588\n",
      "Epoch 108, Loss: 0.0601\n",
      "Epoch 109, Loss: 0.0580\n",
      "Epoch 110, Loss: 0.0590\n",
      "Epoch 111, Loss: 0.0609\n",
      "Epoch 112, Loss: 0.0621\n",
      "Epoch 113, Loss: 0.0552\n",
      "Epoch 114, Loss: 0.0638\n",
      "Epoch 115, Loss: 0.0727\n",
      "Epoch 116, Loss: 0.0531\n",
      "Epoch 117, Loss: 0.0644\n",
      "Epoch 118, Loss: 0.0873\n",
      "Epoch 119, Loss: 0.0699\n",
      "Epoch 120, Loss: 0.0608\n",
      "Epoch 121, Loss: 0.0994\n",
      "Epoch 122, Loss: 0.1125\n",
      "Epoch 123, Loss: 0.0995\n",
      "Epoch 124, Loss: 0.0848\n",
      "Epoch 125, Loss: 0.0684\n",
      "Epoch 126, Loss: 0.0928\n",
      "Epoch 127, Loss: 0.0632\n",
      "Epoch 128, Loss: 0.0751\n",
      "Epoch 129, Loss: 0.1113\n",
      "Epoch 130, Loss: 0.1234\n",
      "Epoch 131, Loss: 0.2005\n",
      "Epoch 132, Loss: 0.1868\n",
      "Epoch 133, Loss: 0.2156\n",
      "Epoch 134, Loss: 0.2057\n",
      "Epoch 135, Loss: 0.1346\n",
      "Epoch 136, Loss: 0.1049\n",
      "Early stopping at epoch 136\n",
      "\n",
      "📊 Model Performance:\n",
      "✅ Accuracy: 0.9143 (±0.0167)\n",
      "✅ Precision: 0.9143 (±0.0500)\n",
      "✅ Recall: 0.9143 (±0.0167)\n",
      "✅ F1-score: 0.9143 (±0.0278)\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T15:00:48.651832Z",
     "start_time": "2025-04-11T15:00:12.461764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Get unique activities from the dataset\n",
    "unique_activities = sorted(set(sequences_df.values.flatten()))\n",
    "\n",
    "# Generate all possible transitions\n",
    "all_possible_transitions = {(a, b) for a in unique_activities for b in unique_activities}\n",
    "\n",
    "# Create a list to store transition count dictionaries\n",
    "transition_counts = []\n",
    "repeat_pattern_features = []\n",
    "\n",
    "# Iterate through each row to count transitions and compute repeat features\n",
    "for _, row in sequences_df.iterrows():\n",
    "    transitions = defaultdict(int)\n",
    "    activities = row.dropna().values  # Non-null activities\n",
    "    \n",
    "    # --- Transition Counting ---\n",
    "    for i in range(len(activities) - 1):\n",
    "        transition = (activities[i], activities[i + 1])\n",
    "        transitions[transition] += 1\n",
    "    row_counts = {t: transitions.get(t, 0) for t in all_possible_transitions}\n",
    "    transition_counts.append(row_counts)\n",
    "    \n",
    "    # --- Repeat Pattern Features ---\n",
    "    max_run = 1\n",
    "    current_run = 1\n",
    "    run_lengths = []\n",
    "    repetitive_activities = set()\n",
    "    \n",
    "    for i in range(1, len(activities)):\n",
    "        if activities[i] == activities[i - 1]:\n",
    "            current_run += 1\n",
    "            repetitive_activities.add(activities[i])\n",
    "        else:\n",
    "            run_lengths.append(current_run)\n",
    "            current_run = 1\n",
    "    run_lengths.append(current_run)  # Add final run\n",
    "    \n",
    "    max_run_length = max(run_lengths)\n",
    "    avg_run_length = np.mean(run_lengths)\n",
    "    num_runs = len(run_lengths)\n",
    "    num_repetitive_activities = len(repetitive_activities)\n",
    "\n",
    "    repeat_pattern_features.append({\n",
    "        'max_run_length': max_run_length,\n",
    "        'avg_run_length': avg_run_length,\n",
    "        'num_runs': num_runs,\n",
    "        'num_repetitive_activities': num_repetitive_activities\n",
    "    })\n",
    "\n",
    "# Convert to DataFrames\n",
    "transitions_df = pd.DataFrame(transition_counts)\n",
    "transitions_df.columns = [f\"{a}->{b}\" for a, b in transitions_df.columns]\n",
    "\n",
    "repeat_df = pd.DataFrame(repeat_pattern_features)\n",
    "\n",
    "# Merge everything\n",
    "result_df = pd.concat([sequences_df, transitions_df, repeat_df], axis=1)\n",
    "\n",
    "# Compute mutual information scores for repeat pattern features\n",
    "mi_scores = mutual_info_classif(repeat_df, result_df['next_activity'], discrete_features=True)\n",
    "feature_scores = dict(zip(repeat_df.columns, mi_scores))\n",
    "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nMutual Information Scores for Repeat Pattern Features:\")\n",
    "for feature, score in sorted_features:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "# Remove the least important features based on MI scores (i.e., num_repetitive_activities, max_run_length)\n",
    "repeat_df = repeat_df.drop(columns=['num_repetitive_activities', 'max_run_length'])\n",
    "\n",
    "# Merge updated repeat_df with result_df\n",
    "result_df = pd.concat([sequences_df, transitions_df, repeat_df], axis=1)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = result_df.drop(columns=['next_activity'])\n",
    "y = result_df['next_activity']\n",
    "\n",
    "X.columns = [col.replace(\":\", \"_\") for col in X.columns]\n",
    "\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "if rare_classes:\n",
    "    if len(rare_classes) > 1:\n",
    "        # Replace multiple rare classes with the last valid label\n",
    "        new_label = len(y.unique()) - 1  \n",
    "        y = y.replace(rare_classes, new_label)\n",
    "    else:\n",
    "        # Duplicate the single rare class in both X and y\n",
    "        rare_indices = y[y.isin(rare_classes)].index  # Get indices of rare classes\n",
    "        \n",
    "        # Duplicate entries in X and y using the rare indices\n",
    "        X = pd.concat([X, X.loc[rare_indices]], ignore_index=True)  # Concatenate rows for X\n",
    "        y = pd.concat([y, y.iloc[rare_indices]], ignore_index=True)  # Concatenate labels for y\n",
    "\n",
    "# Feature selection (now after handling rare classes)\n",
    "X_selected = SelectKBest(mutual_info_classif, k=20).fit_transform(X, y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_selected, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 20\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_activities, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(ActivityTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1, d_model))  # Initialize with dummy sequence length\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_activities)  # Output size matches num_activities\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get sequence length dynamically during the forward pass\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Adjust the positional embedding to match the sequence length\n",
    "        pos_embedding = self.pos_embedding[:, :seq_length, :].expand(x.size(0), seq_length, -1)\n",
    "\n",
    "        # Add embedding and positional encoding\n",
    "        x = self.embedding(x) + pos_embedding\n",
    "\n",
    "        # Transformer layers\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Pooling\n",
    "        x = x.mean(dim=1)  # Pooling over sequence length (average)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)  # Output\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define Model\n",
    "num_activities = X_tensor.max().item() + 1  \n",
    "model = ActivityTransformer(num_activities)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early Stopping\n",
    "best_loss = float('inf')\n",
    "patience, patience_counter = 20, 0  # Stop if no improvement after 20 epochs\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0  # Reset patience\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break  # Stop training\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation (Accuracy, Precision, Recall, F1, SD)\n",
    "all_y_true, all_y_pred = [], []\n",
    "\n",
    "batch_acc, batch_prec, batch_rec, batch_f1 = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        batch_true = y_batch.cpu().numpy()\n",
    "        batch_pred = predicted.cpu().numpy()\n",
    "\n",
    "        # Store all predictions\n",
    "        all_y_true.extend(batch_true)\n",
    "        all_y_pred.extend(batch_pred)\n",
    "\n",
    "        # Compute metrics for the batch\n",
    "        batch_acc.append(accuracy_score(batch_true, batch_pred))\n",
    "        batch_prec.append(precision_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_rec.append(recall_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_f1.append(f1_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "# Compute Overall Metrics\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# Compute Standard Deviation (SD) for each metric\n",
    "acc_std = np.std(batch_acc)\n",
    "prec_std = np.std(batch_prec)\n",
    "rec_std = np.std(batch_rec)\n",
    "f1_std = np.std(batch_f1)\n",
    "\n",
    "# Print Metrics with SD\n",
    "print(\"\\n📊 Model Performance:\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f} (±{acc_std:.4f})\")\n",
    "print(f\"✅ Precision: {precision:.4f} (±{prec_std:.4f})\")\n",
    "print(f\"✅ Recall: {recall:.4f} (±{rec_std:.4f})\")\n",
    "print(f\"✅ F1-score: {f1:.4f} (±{f1_std:.4f})\")\n"
   ],
   "id": "5cda01fbc9fc1d67",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6706363/PycharmProjects/PPM_NextResource/.venv/lib/python3.10/site-packages/sklearn/metrics/cluster/_supervised.py:59: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mutual Information Scores for Repeat Pattern Features:\n",
      "max_run_length: 1.2873\n",
      "num_runs: 1.1414\n",
      "avg_run_length: 1.1414\n",
      "num_repetitive_activities: 0.4906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/6706363/PycharmProjects/PPM_NextResource/.venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4757\n",
      "Epoch 2, Loss: 0.5421\n",
      "Epoch 3, Loss: 0.3876\n",
      "Epoch 4, Loss: 0.3224\n",
      "Epoch 5, Loss: 0.2909\n",
      "Epoch 6, Loss: 0.2919\n",
      "Epoch 7, Loss: 0.3091\n",
      "Epoch 8, Loss: 0.2674\n",
      "Epoch 9, Loss: 0.2797\n",
      "Epoch 10, Loss: 0.2577\n",
      "Epoch 11, Loss: 0.2644\n",
      "Epoch 12, Loss: 0.2536\n",
      "Epoch 13, Loss: 0.2643\n",
      "Epoch 14, Loss: 0.2551\n",
      "Epoch 15, Loss: 0.2490\n",
      "Epoch 16, Loss: 0.2482\n",
      "Epoch 17, Loss: 0.2588\n",
      "Epoch 18, Loss: 0.2509\n",
      "Epoch 19, Loss: 0.2413\n",
      "Epoch 20, Loss: 0.2395\n",
      "Epoch 21, Loss: 0.2373\n",
      "Epoch 22, Loss: 0.2439\n",
      "Epoch 23, Loss: 0.2453\n",
      "Epoch 24, Loss: 0.2378\n",
      "Epoch 25, Loss: 0.2462\n",
      "Epoch 26, Loss: 0.2489\n",
      "Epoch 27, Loss: 0.2488\n",
      "Epoch 28, Loss: 0.2611\n",
      "Epoch 29, Loss: 0.2460\n",
      "Epoch 30, Loss: 0.2401\n",
      "Epoch 31, Loss: 0.2367\n",
      "Epoch 32, Loss: 0.2370\n",
      "Epoch 33, Loss: 0.2447\n",
      "Epoch 34, Loss: 0.2282\n",
      "Epoch 35, Loss: 0.2407\n",
      "Epoch 36, Loss: 0.2362\n",
      "Epoch 37, Loss: 0.2359\n",
      "Epoch 38, Loss: 0.2294\n",
      "Epoch 39, Loss: 0.2288\n",
      "Epoch 40, Loss: 0.2377\n",
      "Epoch 41, Loss: 0.2319\n",
      "Epoch 42, Loss: 0.2316\n",
      "Epoch 43, Loss: 0.2249\n",
      "Epoch 44, Loss: 0.2279\n",
      "Epoch 45, Loss: 0.2478\n",
      "Epoch 46, Loss: 0.2381\n",
      "Epoch 47, Loss: 0.2305\n",
      "Epoch 48, Loss: 0.2354\n",
      "Epoch 49, Loss: 0.2310\n",
      "Epoch 50, Loss: 0.2336\n",
      "Epoch 51, Loss: 0.2394\n",
      "Epoch 52, Loss: 0.2302\n",
      "Epoch 53, Loss: 0.2332\n",
      "Epoch 54, Loss: 0.2347\n",
      "Epoch 55, Loss: 0.2290\n",
      "Epoch 56, Loss: 0.2349\n",
      "Epoch 57, Loss: 0.2287\n",
      "Epoch 58, Loss: 0.2376\n",
      "Epoch 59, Loss: 0.2352\n",
      "Epoch 60, Loss: 0.2232\n",
      "Epoch 61, Loss: 0.2261\n",
      "Epoch 62, Loss: 0.2383\n",
      "Epoch 63, Loss: 0.2213\n",
      "Epoch 64, Loss: 0.2305\n",
      "Epoch 65, Loss: 0.2199\n",
      "Epoch 66, Loss: 0.2246\n",
      "Epoch 67, Loss: 0.2259\n",
      "Epoch 68, Loss: 0.2313\n",
      "Epoch 69, Loss: 0.2338\n",
      "Epoch 70, Loss: 0.2229\n",
      "Epoch 71, Loss: 0.2291\n",
      "Epoch 72, Loss: 0.2300\n",
      "Epoch 73, Loss: 0.2310\n",
      "Epoch 74, Loss: 0.2361\n",
      "Epoch 75, Loss: 0.2288\n",
      "Epoch 76, Loss: 0.2441\n",
      "Epoch 77, Loss: 0.2247\n",
      "Epoch 78, Loss: 0.2241\n",
      "Epoch 79, Loss: 0.2284\n",
      "Epoch 80, Loss: 0.2219\n",
      "Epoch 81, Loss: 0.2288\n",
      "Epoch 82, Loss: 0.2231\n",
      "Epoch 83, Loss: 0.2194\n",
      "Epoch 84, Loss: 0.2207\n",
      "Epoch 85, Loss: 0.2157\n",
      "Epoch 86, Loss: 0.2229\n",
      "Epoch 87, Loss: 0.2211\n",
      "Epoch 88, Loss: 0.2225\n",
      "Epoch 89, Loss: 0.2221\n",
      "Epoch 90, Loss: 0.2276\n",
      "Epoch 91, Loss: 0.2197\n",
      "Epoch 92, Loss: 0.2290\n",
      "Epoch 93, Loss: 0.2259\n",
      "Epoch 94, Loss: 0.2180\n",
      "Epoch 95, Loss: 0.2259\n",
      "Epoch 96, Loss: 0.2179\n",
      "Epoch 97, Loss: 0.2256\n",
      "Epoch 98, Loss: 0.2173\n",
      "Epoch 99, Loss: 0.2188\n",
      "Epoch 100, Loss: 0.2176\n",
      "Epoch 101, Loss: 0.2212\n",
      "Epoch 102, Loss: 0.2209\n",
      "Epoch 103, Loss: 0.2195\n",
      "Epoch 104, Loss: 0.2326\n",
      "Epoch 105, Loss: 0.2172\n",
      "Early stopping at epoch 105\n",
      "\n",
      "📊 Model Performance:\n",
      "✅ Accuracy: 0.9429 (±0.0083)\n",
      "✅ Precision: 0.9167 (±0.0017)\n",
      "✅ Recall: 0.9429 (±0.0083)\n",
      "✅ F1-score: 0.9292 (±0.0120)\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a363b981439e812"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
