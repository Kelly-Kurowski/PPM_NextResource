{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "import pm4py",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    event_log = pm4py.convert_to_dataframe(log)\n",
    "\n",
    "    return event_log\n",
    "\n",
    "event_log = import_xes(\"/Users/6706363/Downloads/BPI_Challenge_2019.xes\")"
   ],
   "id": "5c03dea14064b576",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = event_log[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "\n",
    "df = df.sort_values(by=['org:resource', 'time:timestamp'])\n",
    "\n",
    "df.head(n=20)"
   ],
   "id": "bb3ed39dec015998",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "prefix_length = 700  \n",
    "\n",
    "# Function to create activity sequences\n",
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences, next_activities, resources = [], [], []\n",
    "\n",
    "    for resource, resource_df in df.groupby('org:resource'):\n",
    "        activities = resource_df['concept:name'].values  \n",
    "        if len(activities) >= prefix_length + 1:\n",
    "            prefix = activities[:prefix_length]\n",
    "            next_activity = activities[prefix_length]\n",
    "            sequences.append(prefix)\n",
    "            next_activities.append(next_activity)\n",
    "            resources.append(resource)\n",
    "\n",
    "    sequences_df = pd.DataFrame(sequences, columns=[f\"activity_{i+1}\" for i in range(prefix_length)])\n",
    "    sequences_df['next_activity'] = next_activities\n",
    "    sequences_df['org:resource'] = resources  \n",
    "\n",
    "    return sequences_df\n",
    "\n",
    "# Create sequences\n",
    "sequences_df = create_activity_sequences(df, prefix_length)"
   ],
   "id": "736894a820fc7294",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_encoder = LabelEncoder()\n",
    "all_activities = sequences_df[[f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']].values.flatten()\n",
    "label_encoder.fit(all_activities)\n",
    "\n",
    "# Apply encoding\n",
    "for col in [f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']:\n",
    "    sequences_df[col] = label_encoder.transform(sequences_df[col])\n",
    "\n",
    "# Store mapping\n",
    "activity_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "inverse_activity_mapping = {v: k for k, v in activity_mapping.items()}"
   ],
   "id": "a3cab09085226a06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Experiment 1\n",
    "X = sequences_df[[f\"activity_{i+1}\" for i in range(prefix_length)]]\n",
    "y = sequences_df['next_activity']\n",
    "\n",
    "X.columns = [col.replace(\":\", \"_\") for col in X.columns]\n",
    "\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "if rare_classes:\n",
    "    if len(rare_classes) > 1:\n",
    "        # Replace multiple rare classes with the last valid label\n",
    "        new_label = len(y.unique()) - 1  \n",
    "        y = y.replace(rare_classes, new_label)\n",
    "    else:\n",
    "        # Duplicate the single rare class in both X and y\n",
    "        rare_indices = y[y.isin(rare_classes)].index  # Get indices of rare classes\n",
    "        \n",
    "        # Duplicate entries in X and y using the rare indices\n",
    "        X = pd.concat([X, X.loc[rare_indices]], ignore_index=True)  # Concatenate rows for X\n",
    "        y = pd.concat([y, y.iloc[rare_indices]], ignore_index=True)  # Concatenate labels for y\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 20\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_activities, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(ActivityTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, prefix_length, d_model))\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_activities)  # âœ… Fix: Output size matches num_activities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding  # Add positional encoding\n",
    "        x = self.transformer(x)  \n",
    "        x = x.mean(dim=1)  # Pooling\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        return x\n",
    "\n",
    "# Define Model\n",
    "num_activities = len(activity_mapping)  \n",
    "model = ActivityTransformer(num_activities)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early Stopping\n",
    "best_loss = float('inf')\n",
    "patience, patience_counter = 20, 0  # Stop if no improvement after 20 epochs\n",
    "\n",
    "#Training Loop with Early Stopping\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0  # Reset patience\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break  # Stop training\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation (Accuracy, Precision, Recall, F1, SD)\n",
    "all_y_true, all_y_pred = [], []\n",
    "\n",
    "batch_acc, batch_prec, batch_rec, batch_f1 = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        batch_true = y_batch.cpu().numpy()\n",
    "        batch_pred = predicted.cpu().numpy()\n",
    "\n",
    "        # Store all predictions\n",
    "        all_y_true.extend(batch_true)\n",
    "        all_y_pred.extend(batch_pred)\n",
    "\n",
    "        # Compute metrics for the batch\n",
    "        batch_acc.append(accuracy_score(batch_true, batch_pred))\n",
    "        batch_prec.append(precision_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_rec.append(recall_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_f1.append(f1_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "# Compute Overall Metrics\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# Compute Standard Deviation (SD) for each metric\n",
    "acc_std = np.std(batch_acc)\n",
    "prec_std = np.std(batch_prec)\n",
    "rec_std = np.std(batch_rec)\n",
    "f1_std = np.std(batch_f1)\n",
    "\n",
    "# Print Metrics with SD\n",
    "print(\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f} (Â±{acc_std:.4f})\")\n",
    "print(f\"âœ… Precision: {precision:.4f} (Â±{prec_std:.4f})\")\n",
    "print(f\"âœ… Recall: {recall:.4f} (Â±{rec_std:.4f})\")\n",
    "print(f\"âœ… F1-score: {f1:.4f} (Â±{f1_std:.4f})\")\n"
   ],
   "id": "6591c0b4fe5a52f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Experiment 2: Next Activity Prediction with activity information\n",
    "import binary_classifier\n",
    "\n",
    "ra_diversity_matrix = binary_classifier.create_diversity_matrix(event_log)\n",
    "ra_diversity_matrix_binary = ra_diversity_matrix.copy()\n",
    "# Apply a binary transformation: any count > 0 becomes 1 (yes), else 0 (no)\n",
    "ra_diversity_matrix_binary.iloc[:, 1:] = (ra_diversity_matrix_binary.iloc[:, 1:] > 0).astype(int)\n",
    "\n",
    "activities = ra_diversity_matrix.columns[1:].tolist()  # Convert to a list of activities\n",
    "print(activities)\n",
    "binary_activities = ra_diversity_matrix_binary.iloc[:, :]\n"
   ],
   "id": "1cd1cfa897cd70ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Keep only resources that are in sequences_df\n",
    "filtered_binary_activities = binary_activities[binary_activities['org:resource'].isin(sequences_df['org:resource'])]\n",
    "\n",
    "# Reset index to ensure proper alignment\n",
    "filtered_binary_activities = filtered_binary_activities.reset_index(drop=True)\n",
    "sequences_df = sequences_df.reset_index(drop=True)\n",
    "\n",
    "# Merge again\n",
    "merged_df = pd.concat([sequences_df, filtered_binary_activities], axis=1)"
   ],
   "id": "e22a8c8edbe37b2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "X = merged_df[[f\"activity_{i+1}\" for i in range(prefix_length)] + activities]\n",
    "y = merged_df['next_activity']\n",
    "\n",
    "X.columns = [col.replace(\":\", \"_\") for col in X.columns]\n",
    "\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "if rare_classes:\n",
    "    if len(rare_classes) > 1:\n",
    "        # Replace multiple rare classes with the last valid label\n",
    "        new_label = len(y.unique()) - 1  \n",
    "        y = y.replace(rare_classes, new_label)\n",
    "    else:\n",
    "        # Duplicate the single rare class in both X and y\n",
    "        rare_indices = y[y.isin(rare_classes)].index  # Get indices of rare classes\n",
    "        \n",
    "        # Duplicate entries in X and y using the rare indices\n",
    "        X = pd.concat([X, X.loc[rare_indices]], ignore_index=True)  # Concatenate rows for X\n",
    "        y = pd.concat([y, y.iloc[rare_indices]], ignore_index=True)  # Concatenate labels for y\n",
    "\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X.values, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 20\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_activities, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(ActivityTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, prefix_length + len(activities), d_model))\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_activities)  # âœ… Fix: Output size matches num_activities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding  # Add positional encoding\n",
    "        x = self.transformer(x)  \n",
    "        x = x.mean(dim=1)  # Pooling\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        return x\n",
    "\n",
    "# Define Model\n",
    "num_activities = len(activity_mapping)  # âœ… Ensure correct number of activities\n",
    "model = ActivityTransformer(num_activities)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early Stopping\n",
    "best_loss = float('inf')\n",
    "patience, patience_counter = 20, 0  # Stop if no improvement after 20 epochs\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0  # Reset patience\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break  # Stop training\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation (Accuracy, Precision, Recall, F1, SD)\n",
    "all_y_true, all_y_pred = [], []\n",
    "\n",
    "batch_acc, batch_prec, batch_rec, batch_f1 = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        batch_true = y_batch.cpu().numpy()\n",
    "        batch_pred = predicted.cpu().numpy()\n",
    "\n",
    "        # Store all predictions\n",
    "        all_y_true.extend(batch_true)\n",
    "        all_y_pred.extend(batch_pred)\n",
    "\n",
    "        # Compute metrics for the batch\n",
    "        batch_acc.append(accuracy_score(batch_true, batch_pred))\n",
    "        batch_prec.append(precision_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_rec.append(recall_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_f1.append(f1_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "# Compute Overall Metrics\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# Compute Standard Deviation (SD) for each metric\n",
    "acc_std = np.std(batch_acc)\n",
    "prec_std = np.std(batch_prec)\n",
    "rec_std = np.std(batch_rec)\n",
    "f1_std = np.std(batch_f1)\n",
    "\n",
    "# Print Metrics with SD\n",
    "print(\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f} (Â±{acc_std:.4f})\")\n",
    "print(f\"âœ… Precision: {precision:.4f} (Â±{prec_std:.4f})\")\n",
    "print(f\"âœ… Recall: {recall:.4f} (Â±{rec_std:.4f})\")\n",
    "print(f\"âœ… F1-score: {f1:.4f} (Â±{f1_std:.4f})\")\n"
   ],
   "id": "9b62f14cafdfd47b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sequences_df = sequences_df.drop(columns=['org:resource'])",
   "id": "55c96e80ecffa90b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "\n",
    "# Get unique activities from the dataset\n",
    "unique_activities = sorted(set(sequences_df.values.flatten()))\n",
    "\n",
    "# Generate all possible transitions\n",
    "all_possible_transitions = {(a, b) for a in unique_activities for b in unique_activities}\n",
    "\n",
    "# Create a list to store transition count dictionaries\n",
    "transition_counts = []\n",
    "\n",
    "# Iterate through each row to count transitions\n",
    "for _, row in sequences_df.iterrows():\n",
    "    transitions = defaultdict(int)\n",
    "    activities = row.dropna().values  # Extract non-null activities\n",
    "\n",
    "    # Count actual transitions\n",
    "    for i in range(len(activities) - 1):\n",
    "        transition = (activities[i], activities[i + 1])\n",
    "        transitions[transition] += 1\n",
    "\n",
    "    # Ensure every possible transition exists (fill with 0 if not present)\n",
    "    row_counts = {t: transitions.get(t, 0) for t in all_possible_transitions}\n",
    "    transition_counts.append(row_counts)\n",
    "\n",
    "# Convert list of transition count dictionaries to a DataFrame\n",
    "transitions_df = pd.DataFrame(transition_counts)\n",
    "\n",
    "# Rename columns to string format (e.g., '0->0', '0->1', etc.)\n",
    "transitions_df.columns = [f\"{a}->{b}\" for a, b in transitions_df.columns]\n",
    "\n",
    "# Merge with original DataFrame\n",
    "result_df = pd.concat([sequences_df, transitions_df], axis=1)\n",
    "\n",
    "X = result_df.drop(columns=['next_activity'])\n",
    "y = result_df['next_activity']\n",
    "\n",
    "X.columns = [col.replace(\":\", \"_\") for col in X.columns]\n",
    "\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "if rare_classes:\n",
    "    if len(rare_classes) > 1:\n",
    "        # Replace multiple rare classes with the last valid label\n",
    "        new_label = len(y.unique()) - 1  \n",
    "        y = y.replace(rare_classes, new_label)\n",
    "    else:\n",
    "        # Duplicate the single rare class in both X and y\n",
    "        rare_indices = y[y.isin(rare_classes)].index  # Get indices of rare classes\n",
    "        \n",
    "        # Duplicate entries in X and y using the rare indices\n",
    "        X = pd.concat([X, X.loc[rare_indices]], ignore_index=True)  # Concatenate rows for X\n",
    "        y = pd.concat([y, y.iloc[rare_indices]], ignore_index=True)  # Concatenate labels for y\n",
    "\n",
    "# Feature selection (now after handling rare classes)\n",
    "X_selected = SelectKBest(mutual_info_classif, k=20).fit_transform(X, y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_selected, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Min in X_train: {X_train.min()}, Max in X_train: {X_train.max()}\")\n",
    "print(f\"Min in X_test: {X_test.min()}, Max in X_test: {X_test.max()}\")\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 20\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_activities, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(ActivityTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, X_batch.size(1), d_model))  # Use X_batch.size(1)\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_activities)  # âœ… Fix: Output size matches num_activities\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) + self.pos_embedding  # Add positional encoding\n",
    "        x = self.transformer(x)  \n",
    "        x = x.mean(dim=1)  # Pooling\n",
    "        x = self.fc(x)  # Fully connected layer\n",
    "        return x\n",
    "\n",
    "# Define Model\n",
    "num_activities = X_tensor.max().item() + 1  \n",
    "model = ActivityTransformer(num_activities)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early Stopping\n",
    "best_loss = float('inf')\n",
    "patience, patience_counter = 20, 0  # Stop if no improvement after 20 epochs\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0  # Reset patience\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break  # Stop training\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "all_y_true, all_y_pred = [], []\n",
    "\n",
    "batch_acc, batch_prec, batch_rec, batch_f1 = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        batch_true = y_batch.cpu().numpy()\n",
    "        batch_pred = predicted.cpu().numpy()\n",
    "\n",
    "        # Store all predictions\n",
    "        all_y_true.extend(batch_true)\n",
    "        all_y_pred.extend(batch_pred)\n",
    "\n",
    "        # Compute metrics for the batch\n",
    "        batch_acc.append(accuracy_score(batch_true, batch_pred))\n",
    "        batch_prec.append(precision_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_rec.append(recall_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_f1.append(f1_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "# Compute Overall Metrics\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# Compute Standard Deviation (SD) for each metric\n",
    "acc_std = np.std(batch_acc)\n",
    "prec_std = np.std(batch_prec)\n",
    "rec_std = np.std(batch_rec)\n",
    "f1_std = np.std(batch_f1)\n",
    "\n",
    "print(\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f} (Â±{acc_std:.4f})\")\n",
    "print(f\"âœ… Precision: {precision:.4f} (Â±{prec_std:.4f})\")\n",
    "print(f\"âœ… Recall: {recall:.4f} (Â±{rec_std:.4f})\")\n",
    "print(f\"âœ… F1-score: {f1:.4f} (Â±{f1_std:.4f})\")"
   ],
   "id": "1abde873183c6032",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Get unique activities from the dataset\n",
    "unique_activities = sorted(set(sequences_df.values.flatten()))\n",
    "\n",
    "# Generate all possible transitions\n",
    "all_possible_transitions = {(a, b) for a in unique_activities for b in unique_activities}\n",
    "\n",
    "# Create a list to store transition count dictionaries\n",
    "transition_counts = []\n",
    "repeat_pattern_features = []\n",
    "\n",
    "# Iterate through each row to count transitions and compute repeat features\n",
    "for _, row in sequences_df.iterrows():\n",
    "    transitions = defaultdict(int)\n",
    "    activities = row.dropna().values  # Non-null activities\n",
    "    \n",
    "    # Transition Counting \n",
    "    for i in range(len(activities) - 1):\n",
    "        transition = (activities[i], activities[i + 1])\n",
    "        transitions[transition] += 1\n",
    "    row_counts = {t: transitions.get(t, 0) for t in all_possible_transitions}\n",
    "    transition_counts.append(row_counts)\n",
    "    \n",
    "    # Repeat Pattern Features \n",
    "    max_run = 1\n",
    "    current_run = 1\n",
    "    run_lengths = []\n",
    "    repetitive_activities = set()\n",
    "    \n",
    "    for i in range(1, len(activities)):\n",
    "        if activities[i] == activities[i - 1]:\n",
    "            current_run += 1\n",
    "            repetitive_activities.add(activities[i])\n",
    "        else:\n",
    "            run_lengths.append(current_run)\n",
    "            current_run = 1\n",
    "    run_lengths.append(current_run)  # Add final run\n",
    "    \n",
    "    max_run_length = max(run_lengths)\n",
    "    avg_run_length = np.mean(run_lengths)\n",
    "    num_runs = len(run_lengths)\n",
    "    num_repetitive_activities = len(repetitive_activities)\n",
    "\n",
    "    repeat_pattern_features.append({\n",
    "        'max_run_length': max_run_length,\n",
    "        'avg_run_length': avg_run_length,\n",
    "        'num_runs': num_runs,\n",
    "        'num_repetitive_activities': num_repetitive_activities\n",
    "    })\n",
    "\n",
    "# Convert to DataFrames\n",
    "transitions_df = pd.DataFrame(transition_counts)\n",
    "transitions_df.columns = [f\"{a}->{b}\" for a, b in transitions_df.columns]\n",
    "\n",
    "repeat_df = pd.DataFrame(repeat_pattern_features)\n",
    "\n",
    "# Merge everything\n",
    "result_df = pd.concat([sequences_df, transitions_df, repeat_df], axis=1)\n",
    "\n",
    "# Compute mutual information scores for repeat pattern features\n",
    "mi_scores = mutual_info_classif(repeat_df, result_df['next_activity'], discrete_features=True)\n",
    "feature_scores = dict(zip(repeat_df.columns, mi_scores))\n",
    "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nMutual Information Scores for Repeat Pattern Features:\")\n",
    "for feature, score in sorted_features:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "# Remove the least important features based on MI scores (i.e., num_repetitive_activities, max_run_length)\n",
    "repeat_df = repeat_df.drop(columns=['num_repetitive_activities', 'max_run_length'])\n",
    "\n",
    "# Merge updated repeat_df with result_df\n",
    "result_df = pd.concat([sequences_df, transitions_df, repeat_df], axis=1)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = result_df.drop(columns=['next_activity'])\n",
    "y = result_df['next_activity']\n",
    "\n",
    "X.columns = [col.replace(\":\", \"_\") for col in X.columns]\n",
    "\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "if rare_classes:\n",
    "    if len(rare_classes) > 1:\n",
    "        # Replace multiple rare classes with the last valid label\n",
    "        new_label = len(y.unique()) - 1  \n",
    "        y = y.replace(rare_classes, new_label)\n",
    "    else:\n",
    "        # Duplicate the single rare class in both X and y\n",
    "        rare_indices = y[y.isin(rare_classes)].index  # Get indices of rare classes\n",
    "        \n",
    "        # Duplicate entries in X and y using the rare indices\n",
    "        X = pd.concat([X, X.loc[rare_indices]], ignore_index=True)  # Concatenate rows for X\n",
    "        y = pd.concat([y, y.iloc[rare_indices]], ignore_index=True)  # Concatenate labels for y\n",
    "\n",
    "# Feature selection (now after handling rare classes)\n",
    "X_selected = SelectKBest(mutual_info_classif, k=20).fit_transform(X, y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_selected, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 20\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class ActivityTransformer(nn.Module):\n",
    "    def __init__(self, num_activities, d_model=128, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(ActivityTransformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_activities, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1, d_model))  # Initialize with dummy sequence length\n",
    "\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, num_activities)  # Output size matches num_activities\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get sequence length dynamically during the forward pass\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Adjust the positional embedding to match the sequence length\n",
    "        pos_embedding = self.pos_embedding[:, :seq_length, :].expand(x.size(0), seq_length, -1)\n",
    "\n",
    "        # Add embedding and positional encoding\n",
    "        x = self.embedding(x) + pos_embedding\n",
    "\n",
    "        # Transformer layers\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Pooling\n",
    "        x = x.mean(dim=1)  # Pooling over sequence length (average)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)  # Output\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define Model\n",
    "num_activities = X_tensor.max().item() + 1  \n",
    "model = ActivityTransformer(num_activities)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Early Stopping\n",
    "best_loss = float('inf')\n",
    "patience, patience_counter = 20, 0  # Stop if no improvement after 20 epochs\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping Logic\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        patience_counter = 0  # Reset patience\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break  # Stop training\n",
    "\n",
    "# Load Best Model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation (Accuracy, Precision, Recall, F1, SD)\n",
    "all_y_true, all_y_pred = [], []\n",
    "\n",
    "batch_acc, batch_prec, batch_rec, batch_f1 = [], [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        output = model(X_batch)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "        batch_true = y_batch.cpu().numpy()\n",
    "        batch_pred = predicted.cpu().numpy()\n",
    "\n",
    "        # Store all predictions\n",
    "        all_y_true.extend(batch_true)\n",
    "        all_y_pred.extend(batch_pred)\n",
    "\n",
    "        # Compute metrics for the batch\n",
    "        batch_acc.append(accuracy_score(batch_true, batch_pred))\n",
    "        batch_prec.append(precision_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_rec.append(recall_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "        batch_f1.append(f1_score(batch_true, batch_pred, average='weighted', zero_division=0))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_pred = np.array(all_y_pred)\n",
    "\n",
    "# Compute Overall Metrics\n",
    "accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "precision = precision_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(all_y_true, all_y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
    "\n",
    "# Compute Standard Deviation (SD) for each metric\n",
    "acc_std = np.std(batch_acc)\n",
    "prec_std = np.std(batch_prec)\n",
    "rec_std = np.std(batch_rec)\n",
    "f1_std = np.std(batch_f1)\n",
    "\n",
    "# Print Metrics with SD\n",
    "print(\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"âœ… Accuracy: {accuracy:.4f} (Â±{acc_std:.4f})\")\n",
    "print(f\"âœ… Precision: {precision:.4f} (Â±{prec_std:.4f})\")\n",
    "print(f\"âœ… Recall: {recall:.4f} (Â±{rec_std:.4f})\")\n",
    "print(f\"âœ… F1-score: {f1:.4f} (Â±{f1_std:.4f})\")\n"
   ],
   "id": "5cda01fbc9fc1d67",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
