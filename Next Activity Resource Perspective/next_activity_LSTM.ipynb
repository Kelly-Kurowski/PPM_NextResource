{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pm4py\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    event_log = pm4py.convert_to_dataframe(log)\n",
    "\n",
    "    return event_log\n",
    "\n",
    "event_log = import_xes(\"/Users/6706363/Downloads/BPI_Challenge_2019.xes\")"
   ],
   "id": "ee6368015e06d4da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = event_log[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "\n",
    "df = df.sort_values(by=['org:resource', 'time:timestamp'])\n"
   ],
   "id": "7ef4132142d69c02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_activity_sequences(df, prefix_length):\n",
    "    sequences = []\n",
    "    next_activities = []\n",
    "    resources = []\n",
    "\n",
    "    # Iterate through the dataframe, grouped by resource\n",
    "    for resource, resource_df in df.groupby('org:resource'):\n",
    "        activities = resource_df['concept:name'].values  # Get the activities for this resource\n",
    "\n",
    "        # Only generate sequences if there are enough activities for a valid prefix\n",
    "        if len(activities) >= prefix_length + 1:\n",
    "            prefix = activities[:prefix_length]  # Get the first 'prefix_length' activities\n",
    "            next_activity = activities[prefix_length]  # Next activity after the prefix\n",
    "            sequences.append(prefix)\n",
    "            next_activities.append(next_activity)\n",
    "            resources.append(resource)  # Append the resource for each sequence\n",
    "\n",
    "    # Convert to DataFrame with column names indicating the sequence of activities\n",
    "    sequences_df = pd.DataFrame(sequences, columns=[f\"activity_{i + 1}\" for i in range(prefix_length)])\n",
    "    sequences_df['next_activity'] = next_activities  # Add the next activity to the DataFrame\n",
    "    sequences_df['org:resource'] = resources \n",
    "\n",
    "    return sequences_df\n",
    "\n",
    "\n",
    "prefix_length = 700  # You can change this value\n",
    "sequences_df = create_activity_sequences(df, prefix_length)"
   ],
   "id": "b889d44ecc41bc28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit on all unique activity values (including 'next_activity')\n",
    "all_activities = sequences_df[[f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']].values.flatten()\n",
    "\n",
    "label_encoder.fit(all_activities)  # Fit on the entire dataset\n",
    "\n",
    "# Apply the same encoding across all columns\n",
    "for col in [f\"activity_{i+1}\" for i in range(prefix_length)] + ['next_activity']:\n",
    "    sequences_df[col] = label_encoder.transform(sequences_df[col])\n",
    "\n",
    "sequences_df.head()"
   ],
   "id": "9cd7de8370e7a29a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import LSTM, Dense\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "from keras.api.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "## Experiment 1: Next Activity Prediction without activity information\n",
    "\n",
    "# Define features (prefix activities) and target (next_activity)\n",
    "X = sequences_df[[f\"activity_{i+1}\" for i in range(prefix_length)]]\n",
    "y = sequences_df['next_activity']\n",
    "\n",
    "# Identify and handle rare numeric classes\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "# Handle rare class scenario by replacing rare classes with a placeholder value (e.g., -1)\n",
    "if len(rare_classes) > 1:\n",
    "    y = y.replace(rare_classes, -1)  # Replace rare classes with -1 or some placeholder value\n",
    "elif len(rare_classes) == 1:\n",
    "    # Duplicate the rare class to avoid error\n",
    "    y = y.append(pd.Series(rare_classes * 2)).reset_index(drop=True)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# One-hot encode target labels\n",
    "y_encoded = to_categorical(y_encoded)\n",
    "\n",
    "# Reshape features to 3D array for LSTM input\n",
    "X_reshaped = np.array(X)\n",
    "X_reshaped = X_reshaped.reshape((X_reshaped.shape[0], X_reshaped.shape[1], 1))  # (samples, timesteps, features)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Build the LSTM model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(y_encoded.shape[1], activation='softmax'))  # 'softmax' for multi-class classification\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Implement Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize KFold for cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare lists to store metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, val_index in kf.split(X_train, np.argmax(y_train, axis=1)):\n",
    "    X_train_cv, X_val_cv = X_train[train_index], X_train[val_index]\n",
    "    y_train_cv, y_val_cv = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Build and train the model for each fold\n",
    "    model = build_model()\n",
    "    model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32, validation_data=(X_val_cv, y_val_cv), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict(X_val_cv)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "    # Convert one-hot encoded validation labels back to single class values\n",
    "    y_val_decoded = np.argmax(y_val_cv, axis=1)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(y_val_decoded, y_pred)\n",
    "    precision = precision_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate standard deviation for each metric\n",
    "accuracy_sd = np.std(accuracies)\n",
    "precision_sd = np.std(precisions)\n",
    "recall_sd = np.std(recalls)\n",
    "f1_sd = np.std(f1_scores)\n",
    "\n",
    "# Calculate mean for each metric (cross-validation average)\n",
    "accuracy_mean = np.mean(accuracies)\n",
    "precision_mean = np.mean(precisions)\n",
    "recall_mean = np.mean(recalls)\n",
    "f1_mean = np.mean(f1_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Accuracy: {accuracy_mean:.4f} (±{accuracy_sd:.4f})\")\n",
    "print(f\"Mean Precision: {precision_mean:.4f} (±{precision_sd:.4f})\")\n",
    "print(f\"Mean Recall: {recall_mean:.4f} (±{recall_sd:.4f})\")\n",
    "print(f\"Mean F1-Score: {f1_mean:.4f} (±{f1_sd:.4f})\")\n"
   ],
   "id": "3f3ce7955c06e4a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Experiment 2: Next Activity Prediction with activity information\n",
    "import binary_classifier\n",
    "\n",
    "ra_diversity_matrix = binary_classifier.create_diversity_matrix(event_log)\n",
    "ra_diversity_matrix_binary = ra_diversity_matrix.copy()\n",
    "# Apply a binary transformation: any count > 0 becomes 1 (yes), else 0 (no)\n",
    "ra_diversity_matrix_binary.iloc[:, 1:] = (ra_diversity_matrix_binary.iloc[:, 1:] > 0).astype(int)\n",
    "\n",
    "activities = ra_diversity_matrix.columns[1:].tolist()  # Convert to a list of activities\n",
    "\n",
    "binary_activities = ra_diversity_matrix_binary.iloc[:, :]"
   ],
   "id": "752fcbb23903c06b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Keep only resources that are in sequences_df\n",
    "filtered_binary_activities = binary_activities[binary_activities['org:resource'].isin(sequences_df['org:resource'])]\n",
    "\n",
    "# Reset index to ensure proper alignment\n",
    "filtered_binary_activities = filtered_binary_activities.reset_index(drop=True)\n",
    "sequences_df = sequences_df.reset_index(drop=True)\n",
    "\n",
    "# Merge again\n",
    "merged_df = pd.concat([sequences_df, filtered_binary_activities], axis=1)"
   ],
   "id": "6fdf6d69b0cade17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "from keras.api.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "X = merged_df[[f\"activity_{i+1}\" for i in range(prefix_length)] + activities]\n",
    "y = merged_df['next_activity']\n",
    "\n",
    "# Identify and handle rare numeric classes\n",
    "rare_classes = y.value_counts()[y.value_counts() == 1].index.tolist()\n",
    "\n",
    "# Handle rare class scenario by replacing rare classes with a placeholder value (e.g., -1)\n",
    "if len(rare_classes) > 0:\n",
    "    y = y.replace(rare_classes, -1)  # Replace rare classes with -1 or some placeholder value\n",
    "elif len(rare_classes) == 1:\n",
    "    # Duplicate the rare class to avoid error\n",
    "    y = y.append(pd.Series(rare_classes * 2)).reset_index(drop=True)\n",
    "\n",
    "# Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# One-hot encode target labels\n",
    "y_encoded = to_categorical(y_encoded)\n",
    "\n",
    "# Reshape features to 3D array for LSTM input\n",
    "X_reshaped = np.array(X)\n",
    "X_reshaped = X_reshaped.reshape((X_reshaped.shape[0], X_reshaped.shape[1], 1))  # (samples, timesteps, features)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Build the LSTM model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(y_encoded.shape[1], activation='softmax'))  # 'softmax' for multi-class classification\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Implement Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize KFold for cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare lists to store metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, val_index in kf.split(X_train, np.argmax(y_train, axis=1)):\n",
    "    X_train_cv, X_val_cv = X_train[train_index], X_train[val_index]\n",
    "    y_train_cv, y_val_cv = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Build and train the model for each fold\n",
    "    model = build_model()\n",
    "    model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32, validation_data=(X_val_cv, y_val_cv), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict(X_val_cv)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "    # Convert one-hot encoded validation labels back to single class values\n",
    "    y_val_decoded = np.argmax(y_val_cv, axis=1)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(y_val_decoded, y_pred)\n",
    "    precision = precision_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate standard deviation for each metric\n",
    "accuracy_sd = np.std(accuracies)\n",
    "precision_sd = np.std(precisions)\n",
    "recall_sd = np.std(recalls)\n",
    "f1_sd = np.std(f1_scores)\n",
    "\n",
    "# Calculate mean for each metric (cross-validation average)\n",
    "accuracy_mean = np.mean(accuracies)\n",
    "precision_mean = np.mean(precisions)\n",
    "recall_mean = np.mean(recalls)\n",
    "f1_mean = np.mean(f1_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Accuracy: {accuracy_mean:.4f} (±{accuracy_sd:.4f})\")\n",
    "print(f\"Mean Precision: {precision_mean:.4f} (±{precision_sd:.4f})\")\n",
    "print(f\"Mean Recall: {recall_mean:.4f} (±{recall_sd:.4f})\")\n",
    "print(f\"Mean F1-Score: {f1_mean:.4f} (±{f1_sd:.4f})\")"
   ],
   "id": "3346e2bcc0ed51da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "11b4c145a4a4b252"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sequences_df = sequences_df.drop(columns=['org:resource'])",
   "id": "ed97b89a9fb8f5d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "\n",
    "# Get unique activities from the dataset\n",
    "unique_activities = sorted(set(sequences_df.values.flatten()))\n",
    "\n",
    "# Generate all possible transitions\n",
    "all_possible_transitions = {(a, b) for a in unique_activities for b in unique_activities}\n",
    "\n",
    "# Create a list to store transition count dictionaries\n",
    "transition_counts = []\n",
    "\n",
    "# Iterate through each row to count transitions\n",
    "for _, row in sequences_df.iterrows():\n",
    "    transitions = defaultdict(int)\n",
    "    activities = row.dropna().values  # Extract non-null activities\n",
    "\n",
    "    # Count actual transitions\n",
    "    for i in range(len(activities) - 1):\n",
    "        transition = (activities[i], activities[i + 1])\n",
    "        transitions[transition] += 1\n",
    "\n",
    "    # Ensure every possible transition exists (fill with 0 if not present)\n",
    "    row_counts = {t: transitions.get(t, 0) for t in all_possible_transitions}\n",
    "    transition_counts.append(row_counts)\n",
    "\n",
    "# Convert list of transition count dictionaries to a DataFrame\n",
    "transitions_df = pd.DataFrame(transition_counts)\n",
    "\n",
    "# Rename columns to string format (e.g., '0->0', '0->1', etc.)\n",
    "transitions_df.columns = [f\"{a}->{b}\" for a, b in transitions_df.columns]\n",
    "\n",
    "# Merge with original DataFrame\n",
    "result_df = pd.concat([sequences_df, transitions_df], axis=1)\n",
    "\n",
    "X = result_df.drop(columns=['next_activity'])\n",
    "y = result_df['next_activity']\n",
    "\n",
    "# Identify rare classes (fewer than 2 instances)\n",
    "rare_classes = y.value_counts()[y.value_counts() < 2].index.tolist()\n",
    "\n",
    "# Replace rare classes with -1\n",
    "if len(rare_classes) > 0:\n",
    "    y = y.replace(rare_classes, -1)\n",
    "\n",
    "    # Ensure at least two instances of -1 for StratifiedKFold\n",
    "    if (y == -1).sum() == 1:\n",
    "        y = pd.concat([y, pd.Series([-1])], ignore_index=True)\n",
    "        X = pd.concat([X, X.iloc[[0]]], ignore_index=True)  # Duplicate one row in X\n",
    "\n",
    "\n",
    "# One-hot encode target variable\n",
    "y_encoded = pd.get_dummies(y).values  # Convert categorical labels to one-hot encoding\n",
    "\n",
    "# Feature selection\n",
    "X_selected = SelectKBest(mutual_info_classif, k=20).fit_transform(X, y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_encoded, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Reshape X_train and X_test to 3D for LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # (samples, timesteps=1, features)\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[2])))  # Fix input shape\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(y_encoded.shape[1], activation='softmax'))  # Ensure correct output size\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Implement Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize KFold for cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare lists to store metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, val_index in kf.split(X_train, y_train.argmax(axis=1)):\n",
    "    X_train_cv, X_val_cv = X_train[train_index], X_train[val_index]\n",
    "    y_train_cv, y_val_cv = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Build and train the model for each fold\n",
    "    model = build_model()\n",
    "    model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32, validation_data=(X_val_cv, y_val_cv), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict(X_val_cv)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Convert one-hot encoded validation labels back to single class values\n",
    "    y_val_decoded = np.argmax(y_val_cv, axis=1)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(y_val_decoded, y_pred)\n",
    "    precision = precision_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate standard deviation for each metric\n",
    "accuracy_sd = np.std(accuracies)\n",
    "precision_sd = np.std(precisions)\n",
    "recall_sd = np.std(recalls)\n",
    "f1_sd = np.std(f1_scores)\n",
    "\n",
    "# Calculate mean for each metric (cross-validation average)\n",
    "accuracy_mean = np.mean(accuracies)\n",
    "precision_mean = np.mean(precisions)\n",
    "recall_mean = np.mean(recalls)\n",
    "f1_mean = np.mean(f1_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Accuracy: {accuracy_mean:.4f} (±{accuracy_sd:.4f})\")\n",
    "print(f\"Mean Precision: {precision_mean:.4f} (±{precision_sd:.4f})\")\n",
    "print(f\"Mean Recall: {recall_mean:.4f} (±{recall_sd:.4f})\")\n",
    "print(f\"Mean F1-Score: {f1_mean:.4f} (±{f1_sd:.4f})\")\n"
   ],
   "id": "89eab19008e4768f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Experiment 5: Next Activity Prediction with activity transitions count and repeat pattern features\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import LSTM, Dense\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Get unique activities from the dataset\n",
    "unique_activities = sorted(set(sequences_df.values.flatten()))\n",
    "\n",
    "# Generate all possible transitions\n",
    "all_possible_transitions = {(a, b) for a in unique_activities for b in unique_activities}\n",
    "\n",
    "# Create a list to store transition count dictionaries\n",
    "transition_counts = []\n",
    "repeat_pattern_features = []\n",
    "\n",
    "# Iterate through each row to count transitions and compute repeat features\n",
    "for _, row in sequences_df.iterrows():\n",
    "    transitions = defaultdict(int)\n",
    "    activities = row.dropna().values  # Non-null activities\n",
    "    \n",
    "    # --- Transition Counting ---\n",
    "    for i in range(len(activities) - 1):\n",
    "        transition = (activities[i], activities[i + 1])\n",
    "        transitions[transition] += 1\n",
    "    row_counts = {t: transitions.get(t, 0) for t in all_possible_transitions}\n",
    "    transition_counts.append(row_counts)\n",
    "    \n",
    "    # --- Repeat Pattern Features ---\n",
    "    max_run = 1\n",
    "    current_run = 1\n",
    "    run_lengths = []\n",
    "    repetitive_activities = set()\n",
    "    \n",
    "    for i in range(1, len(activities)):\n",
    "        if activities[i] == activities[i - 1]:\n",
    "            current_run += 1\n",
    "            repetitive_activities.add(activities[i])\n",
    "        else:\n",
    "            run_lengths.append(current_run)\n",
    "            current_run = 1\n",
    "    run_lengths.append(current_run)  # Add final run\n",
    "    \n",
    "    max_run_length = max(run_lengths)\n",
    "    avg_run_length = np.mean(run_lengths)\n",
    "    num_runs = len(run_lengths)\n",
    "    num_repetitive_activities = len(repetitive_activities)\n",
    "\n",
    "    repeat_pattern_features.append({\n",
    "        'max_run_length': max_run_length,\n",
    "        'avg_run_length': avg_run_length,\n",
    "        'num_runs': num_runs,\n",
    "        'num_repetitive_activities': num_repetitive_activities\n",
    "    })\n",
    "\n",
    "# Convert to DataFrames\n",
    "transitions_df = pd.DataFrame(transition_counts)\n",
    "transitions_df.columns = [f\"{a}->{b}\" for a, b in transitions_df.columns]\n",
    "\n",
    "repeat_df = pd.DataFrame(repeat_pattern_features)\n",
    "\n",
    "# Merge everything\n",
    "result_df = pd.concat([sequences_df, transitions_df, repeat_df], axis=1)\n",
    "\n",
    "# Compute mutual information scores for repeat pattern features\n",
    "mi_scores = mutual_info_classif(repeat_df, result_df['next_activity'], discrete_features=True)\n",
    "feature_scores = dict(zip(repeat_df.columns, mi_scores))\n",
    "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nMutual Information Scores for Repeat Pattern Features:\")\n",
    "for feature, score in sorted_features:\n",
    "    print(f\"{feature}: {score:.4f}\")\n",
    "\n",
    "# Remove the least important features based on MI scores (i.e., num_repetitive_activities, max_run_length)\n",
    "repeat_df = repeat_df.drop(columns=['num_repetitive_activities', 'max_run_length'])\n",
    "\n",
    "# Merge updated repeat_df with result_df\n",
    "result_df = pd.concat([sequences_df, transitions_df, repeat_df], axis=1)\n",
    "\n",
    "# Prepare features and labels\n",
    "X = result_df.drop(columns=['next_activity'])\n",
    "y = result_df['next_activity']\n",
    "\n",
    "# Identify rare classes (fewer than 2 instances)\n",
    "rare_classes = y.value_counts()[y.value_counts() < 2].index.tolist()\n",
    "\n",
    "# Replace rare classes with -1\n",
    "if len(rare_classes) > 0:\n",
    "    y = y.replace(rare_classes, -1)\n",
    "\n",
    "    # Ensure at least two instances of -1 for StratifiedKFold\n",
    "    if (y == -1).sum() == 1:\n",
    "        y = pd.concat([y, pd.Series([-1])], ignore_index=True)\n",
    "        X = pd.concat([X, X.iloc[[0]]], ignore_index=True)  # Duplicate one row in X\n",
    "        \n",
    "# Feature selection\n",
    "X_selected = SelectKBest(mutual_info_classif, k=20).fit_transform(X, y)\n",
    "\n",
    "# One-hot encode target variable\n",
    "y_encoded = pd.get_dummies(y).values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y_encoded, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Reshape X_train and X_test to 3D for LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))  # (samples, timesteps=1, features)\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build the LSTM model\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, return_sequences=True, input_shape=(1, X_train.shape[2])))  # Fix input shape\n",
    "    model.add(LSTM(50, return_sequences=False))\n",
    "    model.add(Dense(y_encoded.shape[1], activation='softmax'))  # Ensure correct output size\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Implement Early Stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize KFold for cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Prepare lists to store metrics\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, val_index in kf.split(X_train, y_train.argmax(axis=1)):\n",
    "    X_train_cv, X_val_cv = X_train[train_index], X_train[val_index]\n",
    "    y_train_cv, y_val_cv = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Build and train the model for each fold\n",
    "    model = build_model()\n",
    "    model.fit(X_train_cv, y_train_cv, epochs=50, batch_size=32, validation_data=(X_val_cv, y_val_cv), callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_prob = model.predict(X_val_cv)\n",
    "    y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "    \n",
    "    # Convert one-hot encoded validation labels back to single class values\n",
    "    y_val_decoded = np.argmax(y_val_cv, axis=1)\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracy = accuracy_score(y_val_decoded, y_pred)\n",
    "    precision = precision_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_val_decoded, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Append metrics to lists\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Calculate standard deviation for each metric\n",
    "accuracy_sd = np.std(accuracies)\n",
    "precision_sd = np.std(precisions)\n",
    "recall_sd = np.std(recalls)\n",
    "f1_sd = np.std(f1_scores)\n",
    "\n",
    "# Calculate mean for each metric (cross-validation average)\n",
    "accuracy_mean = np.mean(accuracies)\n",
    "precision_mean = np.mean(precisions)\n",
    "recall_mean = np.mean(recalls)\n",
    "f1_mean = np.mean(f1_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Accuracy: {accuracy_mean:.4f} (±{accuracy_sd:.4f})\")\n",
    "print(f\"Mean Precision: {precision_mean:.4f} (±{precision_sd:.4f})\")\n",
    "print(f\"Mean Recall: {recall_mean:.4f} (±{recall_sd:.4f})\")\n",
    "print(f\"Mean F1-Score: {f1_mean:.4f} (±{f1_sd:.4f})\")\n"
   ],
   "id": "be81e00865cf0781",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
